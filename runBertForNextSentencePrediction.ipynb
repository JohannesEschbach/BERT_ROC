{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "runBertForNextSentencePrediction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "taj1zzZAvY1U",
        "outputId": "e23bc391-e92e-4cff-9816-cf27651b892c"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H2vOFbYWdhth",
        "outputId": "00c42138-f348-4e31-d660-7743b84ef283"
      },
      "source": [
        "from google.colab import drive\r\n",
        "\r\n",
        "current_directory = '/content/drive/My Drive/FSem/'\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGUHTlObOfPr"
      },
      "source": [
        "# Headers and Global Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4MUY1yyup2m"
      },
      "source": [
        "import csv\r\n",
        "import torch\r\n",
        "from torch.nn.functional import softmax\r\n",
        "from transformers import BertForNextSentencePrediction, BertTokenizer\r\n",
        "from sklearn.metrics import confusion_matrix\r\n",
        "from sklearn.metrics import classification_report\r\n",
        "from tqdm import tqdm\r\n",
        "from transformers import AdamW\r\n",
        "from transformers import get_linear_schedule_with_warmup\r\n",
        "from matplotlib import pyplot as plt\r\n",
        "\r\n",
        "\r\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\r\n",
        "\r\n",
        "CLOZE_MODEL = 'bertfornsp_cloze_finetuned'\r\n",
        "ROC_MODEL = 'bertfornsp_roc_finetuned'\r\n",
        "# underlying pretrained LM\r\n",
        "BASE_MODEL = 'bert-large-uncased-whole-word-masking'\r\n",
        "\r\n",
        "BATCH_SIZE = 12\r\n",
        "WARMUP_EPOCHS = 1\r\n",
        "TRAIN_EPOCHS = 10\r\n",
        "LAST_EPOCH = -1"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJO2HeysOmYx"
      },
      "source": [
        "# Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdWgHZQtxHi8"
      },
      "source": [
        "class RocStories(torch.utils.data.Dataset):\n",
        "    def __init__(self):    \n",
        "        dataset = []       \n",
        "        with open(current_directory + 'roc_stories.csv', \n",
        "                  'r', encoding='utf-8') as d:\n",
        "            \n",
        "            reader = csv.reader(d, quotechar='\"', delimiter=',',\n",
        "                                quoting=csv.QUOTE_ALL, skipinitialspace=True)                \n",
        "            for line in reader:\n",
        "                dataset.append(line)  \n",
        "\n",
        "        self.data = []\n",
        "        self.labels = []\n",
        "\n",
        "        stories = []\n",
        "        endings = []\n",
        "        for sample in dataset:           \n",
        "            start = \" \".join(sample[2:-1])\n",
        "            stories.append(start)            \n",
        "            end = sample[-1]                        \n",
        "            endings.append(end)\n",
        "\n",
        "        from random import shuffle\n",
        "        wrong_endings = endings.copy()\n",
        "        shuffle(wrong_endings)\n",
        "\n",
        "        assert len(stories) == len(endings)\n",
        "        for i, story in enumerate(stories):\n",
        "            \n",
        "            #True Ending\n",
        "            self.data.append([story, endings[i]])\n",
        "            self.labels.append(0)\n",
        "\n",
        "            #Wrong Ending\n",
        "            self.data.append([story, wrong_endings[i]])\n",
        "            self.labels.append(1)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        X = self.data[idx]\n",
        "        y = self.labels[idx]        \n",
        "        return X, y\n",
        "\n",
        "    def __len__(self):\n",
        "        assert len(self.data) == len(self.labels)\n",
        "        return len(self.labels)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6zAL8cquyF1"
      },
      "source": [
        "class ClozeTest(torch.utils.data.Dataset):\r\n",
        "    def __init__(self, dev=True):\r\n",
        "        \r\n",
        "        dataset = []\r\n",
        "\r\n",
        "        # if dev=True, we load the dev set for testing\r\n",
        "        with open(current_directory + 'cloze_test.csv' if dev else \r\n",
        "                  current_directory + 'cloze_train.csv', \r\n",
        "                  'r', encoding='utf-8') as d:\r\n",
        "            reader = csv.reader(d, quotechar='\"', delimiter=',', \r\n",
        "                                quoting=csv.QUOTE_ALL, skipinitialspace=True)                \r\n",
        "            for line in reader:\r\n",
        "                dataset.append(line) \r\n",
        "\r\n",
        "        self.data = []\r\n",
        "        self.labels = []\r\n",
        "\r\n",
        "        for sample in dataset:\r\n",
        "            \r\n",
        "            start = \" \".join(sample[1:-3])\r\n",
        "            end1 = sample[-3]\r\n",
        "            end2 = sample[-2]\r\n",
        "            right_ending = sample[-1]\r\n",
        "\r\n",
        "            self.data.append([start, end1])\r\n",
        "            self.labels.append(0 if \"1\" == right_ending else 1)\r\n",
        "\r\n",
        "            self.data.append([start, end2])\r\n",
        "            self.labels.append(0 if \"2\" == right_ending else 1)\r\n",
        "\r\n",
        "    def __getitem__(self, idx):\r\n",
        "        X = self.data[idx]\r\n",
        "        y = self.labels[idx]        \r\n",
        "        return X, y\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        assert len(self.data) == len(self.labels)\r\n",
        "        return len(self.labels)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5vXXr3pOpwh"
      },
      "source": [
        "# Auxiliary Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dw0X81uWJVwd"
      },
      "source": [
        "def getModelFileName(model_name, last_epoch):\r\n",
        "    return current_directory + model_name + str(last_epoch)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncUDboxWI3gn"
      },
      "source": [
        "def getLossesFileName(model_name, epoch):\r\n",
        "    return current_directory + model_name + str(epoch) + \"losses.txt\""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1F_fhmrWrZlZ"
      },
      "source": [
        "def getOptimizerFileName(model_name, last_epoch):\r\n",
        "    return current_directory + model_name + str(last_epoch) + \"optimizer.pt\""
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgItyJtueXHR"
      },
      "source": [
        "def weight_diff(model1, model2):\r\n",
        "    model1, model2 = model1.to(\"cpu\"), model2.to(\"cpu\")\r\n",
        "    diff = torch.nn.MSELoss() # diff(a, b) = ((a - b) ** 2).mean()\r\n",
        "\r\n",
        "    xweights, yweights, xbiases, ybiases = dict(), dict(), dict(), dict()\r\n",
        "    layer_names = set()\r\n",
        "\r\n",
        "    for (name, parameter1), parameter2 in zip(\r\n",
        "        model1.bert.encoder.layer.named_parameters(),\r\n",
        "        model2.bert.encoder.layer.parameters()\r\n",
        "    ):\r\n",
        "\r\n",
        "        difference = diff(parameter1, parameter2).item()\r\n",
        "\r\n",
        "        name = name.split(\".\")\r\n",
        "        xtick = float(name[0])\r\n",
        "        layer_name = \".\".join(name[1:-1])\r\n",
        "        parameter_type = name[-1]\r\n",
        "\r\n",
        "        if layer_name not in layer_names:\r\n",
        "            layer_names.add(layer_name)\r\n",
        "            xweights[layer_name], xbiases[layer_name] = list(), list()\r\n",
        "            yweights[layer_name], ybiases[layer_name] = list(), list()\r\n",
        "\r\n",
        "        if parameter_type == \"weight\":\r\n",
        "            yweights[layer_name].append(difference)\r\n",
        "            xweights[layer_name].append(xtick + 0.0)\r\n",
        "        else: # if parameter_type == \"bias\"\r\n",
        "            ybiases[layer_name].append(difference)\r\n",
        "            xbiases[layer_name].append(xtick + 0.5)\r\n",
        "\r\n",
        "    for name in layer_names:\r\n",
        "        plt.bar(xweights[name], yweights[name], width=0.4, label=\"weight\")\r\n",
        "        plt.bar(xbiases[name], ybiases[name], width=0.4, label=\"bias\")\r\n",
        "        plt.xticks(xweights[name])\r\n",
        "        plt.legend()\r\n",
        "        plt.title(name)\r\n",
        "        plt.show()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PECOGcFZOx5h"
      },
      "source": [
        "# Functions for Training and Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yCLlZ7UPpfa"
      },
      "source": [
        "def train(model_file=BASE_MODEL, batch_size=BATCH_SIZE,\r\n",
        "          warmup_epochs=WARMUP_EPOCHS, train_epochs=TRAIN_EPOCHS,\r\n",
        "          last_epoch=LAST_EPOCH, cloze_test=True, verbose=False):\r\n",
        "    \r\n",
        "    tokenizer = BertTokenizer.from_pretrained(BASE_MODEL)\r\n",
        "    model = BertForNextSentencePrediction.from_pretrained(model_file)\r\n",
        "    # The old weights are saved in model_old to be used to compare to model\r\n",
        "    model_old = BertForNextSentencePrediction.from_pretrained(model_file)\r\n",
        "\r\n",
        "    #Send to GPU and allow Training\r\n",
        "    model = model.to(device)\r\n",
        "    model.train()\r\n",
        "\r\n",
        "    trainloader = torch.utils.data.DataLoader(\r\n",
        "        ClozeTest(dev=False) if cloze_test else RocStories(),\r\n",
        "        batch_size=batch_size, shuffle=True\r\n",
        "    )\r\n",
        "\r\n",
        "    #LR maybe needs to be optimized\r\n",
        "    optimizer = AdamW(model.parameters(), lr=1e-5)\r\n",
        "    if last_epoch != -1:\r\n",
        "        optimizer.load_state_dict(\r\n",
        "            torch.load(getOptimizerFileName(model_file, last_epoch))\r\n",
        "        )\r\n",
        "    n_batches =  len(trainloader)\r\n",
        "    scheduler = get_linear_schedule_with_warmup(\r\n",
        "        optimizer,\r\n",
        "        num_warmup_steps=(warmup_epochs * n_batches),\r\n",
        "        num_training_steps=(train_epochs * n_batches),\r\n",
        "        last_epoch=(last_epoch + 1) * n_batches - 1 # actually, last_step\r\n",
        "    )\r\n",
        "\r\n",
        "    epochs_range = range(last_epoch + 1, train_epochs)\r\n",
        "    for epoch in tqdm(epochs_range):\r\n",
        "        losses = []\r\n",
        "        \r\n",
        "        for batchId, (stories, labels) in zip(range(n_batches), trainloader):\r\n",
        "            # this is PyTorch-specific as gradients get accumulated        \r\n",
        "            optimizer.zero_grad()\r\n",
        "\r\n",
        "            start = stories[0]\r\n",
        "            end = stories[1]\r\n",
        "\r\n",
        "            labels = labels.to(device)\r\n",
        "           \r\n",
        "            # Tokenize sentence pairs.\r\n",
        "            # All sequences in batch processing must be same length.\r\n",
        "            # Therefore we use padding to fill shorter sequences\r\n",
        "            # with uninterpreted [PAD] tokens)\r\n",
        "            tokenized_batch = tokenizer(start, padding = True, text_pair = end,\r\n",
        "                                        return_tensors='pt').to(device)\r\n",
        "            \r\n",
        "            loss = model(**tokenized_batch, labels = labels).loss\r\n",
        "            if verbose:\r\n",
        "                print(\"Epoch \" + str(epoch + 1) + \r\n",
        "                      \" Batch \" + batchId + \" of \" + n_batches + \r\n",
        "                      \" Loss: \" + loss.item())\r\n",
        "            losses.append(loss.item())\r\n",
        "\r\n",
        "            loss.backward()\r\n",
        "            optimizer.step()\r\n",
        "            scheduler.step() # Huggingface specific: step = epoch\r\n",
        "\r\n",
        "        # Save model parameters in a file after the epoch\r\n",
        "        model.save_pretrained(getModelFileName((CLOZE_MODEL if cloze_test\r\n",
        "                                                else ROC_MODEL), epoch + 1))\r\n",
        "        # Save optimizer state after the epoch\r\n",
        "        torch.save(optimizer.state_dict(),\r\n",
        "                   getOptimizerFileName((CLOZE_MODEL if cloze_test\r\n",
        "                                         else ROC_MODEL), epoch + 1))\r\n",
        "        # Save the losses during the epoch\r\n",
        "        with open(getLossesFileName((CLOZE_MODEL if cloze_test  else ROC_MODEL),\r\n",
        "                                   epoch), 'w') as f:\r\n",
        "            f.write(\"\\n\".join([str(loss) for loss in losses]))\r\n",
        "    \r\n",
        "    # Losses are gathered for plotting\r\n",
        "    losses = []\r\n",
        "    for epoch in range(train_epochs):\r\n",
        "        with open(getLossesFileName((CLOZE_MODEL if cloze_test  else ROC_MODEL),\r\n",
        "                                   epoch), 'r') as f:\r\n",
        "            for line in f:\r\n",
        "                losses.append(float(line))\r\n",
        "    # Loss function change over steps is plotted below.\r\n",
        "    plt.plot(losses)\r\n",
        "    plt.xticks(\r\n",
        "        ticks=[i * n_batches for i in epochs_range],\r\n",
        "        labels=epochs_range\r\n",
        "    )\r\n",
        "    plt.title((\"Story Cloze\" if cloze_test else \"ROCStories\") + \" Training\")\r\n",
        "    plt.xlabel(\"Epoch\")\r\n",
        "    plt.ylabel(\"Loss\")\r\n",
        "    plt.show()\r\n",
        "\r\n",
        "    # Models are compared\r\n",
        "    weight_diff(model, model_old)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypcXSNFzu0Ds"
      },
      "source": [
        "def test(model_file=BASE_MODEL, verbose = False):\r\n",
        "    softmax = torch.nn.Softmax(dim=1)\r\n",
        "    tokenizer = BertTokenizer.from_pretrained(BASE_MODEL)\r\n",
        "    model = BertForNextSentencePrediction.from_pretrained(model_file)\r\n",
        "\r\n",
        "    #Send to GPU and allow Evaluation\r\n",
        "    model = model.to(device)\r\n",
        "    model.eval()\r\n",
        "\r\n",
        "    #Dataloader\r\n",
        "    devloader = torch.utils.data.DataLoader(ClozeTest(), batch_size=10)\r\n",
        "\r\n",
        "    pred_list, label_list = list(), list()\r\n",
        "\r\n",
        "    for stories, labels in tqdm(devloader, disable=verbose):\r\n",
        "        \r\n",
        "        start = stories[0]\r\n",
        "        end = stories[1]\r\n",
        "        \r\n",
        "        # Tokenize sentence pairs.\r\n",
        "        # All sequences in batch processing must be same length.\r\n",
        "        # Therefore we use padding to fill shorter sequences\r\n",
        "        # with uninterpreted [PAD] tokens)\r\n",
        "        tokenized_batch = tokenizer(start, padding = True, text_pair = end,\r\n",
        "                                    return_tensors='pt').to(device)\r\n",
        "\r\n",
        "        #Send to GPU\r\n",
        "        labels = labels.to(device)\r\n",
        "\r\n",
        "        outputs = model(**tokenized_batch, labels = labels)\r\n",
        "        logits = outputs.logits\r\n",
        "\r\n",
        "        # Model predicts sentence-pair as correct if True-logit > False-logit\r\n",
        "        predictions = logits.argmax(dim=1).int()\r\n",
        "        probs = softmax(logits).cpu().detach()\r\n",
        "\r\n",
        "        # Extra info print() if verbose\r\n",
        "        if verbose:\r\n",
        "            # iterate over elements in batch\r\n",
        "            for i, element_input_ids in enumerate(tokenized_batch.input_ids):\r\n",
        "                print(tokenizer.decode(element_input_ids))\r\n",
        "                print(\"Probability:\", probs[i][0].item() * 100)\r\n",
        "                print(\"Predicted: \", bool(predictions[i]))\r\n",
        "                print(\"True label: \", bool(labels[i]))\r\n",
        "\r\n",
        "        pred_list.extend(predictions.tolist())\r\n",
        "        label_list.extend(labels.tolist())\r\n",
        "\r\n",
        "    print(confusion_matrix(label_list, pred_list))\r\n",
        "    print(classification_report(label_list, pred_list))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lr6l5DZIsLN"
      },
      "source": [
        "# Testing Pretrained BertForNextSentencePrediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEFH-RCZIsWV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6e4cbc4-4e14-45c4-db6d-60f6677836c4"
      },
      "source": [
        "test()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForNextSentencePrediction: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "100%|██████████| 375/375 [00:45<00:00,  8.16it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[ 348 1523]\n",
            " [ 298 1575]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.54      0.19      0.28      1871\n",
            "           1       0.51      0.84      0.63      1873\n",
            "\n",
            "    accuracy                           0.51      3744\n",
            "   macro avg       0.52      0.51      0.46      3744\n",
            "weighted avg       0.52      0.51      0.46      3744\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXgTW9ZGIshN"
      },
      "source": [
        "# Training and Testing the Model on ROCStories Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60O5LnU2IsyG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c55ae11-a7ae-4b5a-b0c8-4c98f34c0a3e"
      },
      "source": [
        "train_epochs_roc = 1\r\n",
        "train(train_epochs=train_epochs_roc, warmup_epochs=0, cloze_test=False)\r\n",
        "test(getModelFileName(ROC_MODEL, train_epochs_roc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForNextSentencePrediction: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForNextSentencePrediction: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EDcfMlhItBd"
      },
      "source": [
        "# Training and Testing the Model Further on StoryCloze Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWa6H3k_ItMV"
      },
      "source": [
        "train_epochs_cloze = 10\r\n",
        "train(model_file=getModelFileName(ROC_MODEL, train_epochs_roc), cloze_test=True)\r\n",
        "test(getModelFileName(CLOZE_MODEL, train_epochs_cloze))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}