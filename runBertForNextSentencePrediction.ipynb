{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "runBertForNextSentencePrediction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "taj1zzZAvY1U",
        "outputId": "1c10e12f-8e07-4f01-e506-888947aedb3e"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f9/54/5ca07ec9569d2f232f3166de5457b63943882f7950ddfcc887732fc7fb23/transformers-4.3.3-py3-none-any.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 10.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/23/2ddc317b2121117bf34dd00f5b0de194158f2a44ee2bf5e47c7166878a97/tokenizers-0.10.1-cp37-cp37m-manylinux2010_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 35.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 34.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp37-none-any.whl size=893262 sha256=328ee4530f23e533a5557ab5168ca73fe1ffbefa2317a0cde871cf4809951324\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.10.1 transformers-4.3.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H2vOFbYWdhth",
        "outputId": "583884a3-b1a9-48b9-e5be-616da59f5dee"
      },
      "source": [
        "from google.colab import drive\r\n",
        "\r\n",
        "currentDirectory = '/content/drive/My Drive/FSem/'\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4MUY1yyup2m"
      },
      "source": [
        "import csv\r\n",
        "import torch\r\n",
        "from torch.nn.functional import softmax\r\n",
        "from transformers import BertForNextSentencePrediction, BertTokenizer\r\n",
        "from sklearn.metrics import confusion_matrix\r\n",
        "from sklearn.metrics import classification_report\r\n",
        "from tqdm import tqdm\r\n",
        "from transformers import AdamW\r\n",
        "from transformers import get_linear_schedule_with_warmup\r\n",
        "\r\n",
        "\r\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\r\n",
        "\r\n",
        "# underlying pretrained LM\r\n",
        "BASE_MODEL = 'bert-large-uncased-whole-word-masking'\r\n",
        "\r\n",
        "BATCH_SIZE = 12\r\n",
        "WARMUP_EPOCHS = 1\r\n",
        "TRAIN_EPOCHS = 10\r\n",
        "LAST_EPOCH = -1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdWgHZQtxHi8"
      },
      "source": [
        "class RocStories(torch.utils.data.Dataset):\n",
        "    def __init__(self):    \n",
        "        dataset = []       \n",
        "        with open(currentDirectory + 'roc_stories.csv', \n",
        "                  'r', encoding='utf-8') as d:\n",
        "            \n",
        "            reader = csv.reader(d, quotechar='\"', delimiter=',',\n",
        "                                quoting=csv.QUOTE_ALL, skipinitialspace=True)                \n",
        "            for line in reader:\n",
        "                dataset.append(line)  \n",
        "\n",
        "        self.data = []\n",
        "        self.labels = []\n",
        "\n",
        "        stories = []\n",
        "        endings = []\n",
        "        for sample in dataset:           \n",
        "            start = \" \".join(sample[2:-1])\n",
        "            stories.append(start)            \n",
        "            end = sample[-1]                        \n",
        "            endings.append(end)\n",
        "\n",
        "        from random import shuffle\n",
        "        wrong_endings = endings.copy()\n",
        "        shuffle(wrong_endings)\n",
        "\n",
        "        assert len(stories) == len(endings)\n",
        "        for i, story in enumerate(stories):\n",
        "            \n",
        "            #True Ending\n",
        "            self.data.append([story, endings[i]])\n",
        "            self.labels.append(0)\n",
        "\n",
        "            #Wrong Ending\n",
        "            self.data.append([story, wrong_endings[i]])\n",
        "            self.labels.append(1)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        X = self.data[idx]\n",
        "        y = self.labels[idx]        \n",
        "        return X, y\n",
        "\n",
        "    def __len__(self):\n",
        "        assert len(self.data) == len(self.labels)\n",
        "        return len(self.labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6zAL8cquyF1"
      },
      "source": [
        "class ClozeTest(torch.utils.data.Dataset):\r\n",
        "    def __init__(self, dev=True):\r\n",
        "        \r\n",
        "        dataset = []\r\n",
        "\r\n",
        "        # if dev=True, we load the dev set for testing\r\n",
        "        with open(currentDirectory + 'cloze_test.csv' if dev else \r\n",
        "                  currentDirectory + 'cloze_train.csv', \r\n",
        "                  'r', encoding='utf-8') as d:\r\n",
        "            reader = csv.reader(d, quotechar='\"', delimiter=',', \r\n",
        "                                quoting=csv.QUOTE_ALL, skipinitialspace=True)                \r\n",
        "            for line in reader:\r\n",
        "                dataset.append(line) \r\n",
        "\r\n",
        "        self.data = []\r\n",
        "        self.labels = []\r\n",
        "\r\n",
        "        for sample in dataset:\r\n",
        "            \r\n",
        "            start = \" \".join(sample[1:-3])\r\n",
        "            end1 = sample[-3]\r\n",
        "            end2 = sample[-2]\r\n",
        "            right_ending = sample[-1]\r\n",
        "\r\n",
        "            self.data.append([start, end1])\r\n",
        "            self.labels.append(0 if \"1\" == right_ending else 1)\r\n",
        "\r\n",
        "            self.data.append([start, end2])\r\n",
        "            self.labels.append(0 if \"2\" == right_ending else 1)\r\n",
        "\r\n",
        "    def __getitem__(self, idx):\r\n",
        "        X = self.data[idx]\r\n",
        "        y = self.labels[idx]        \r\n",
        "        return X, y\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        assert len(self.data) == len(self.labels)\r\n",
        "        return len(self.labels)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yCLlZ7UPpfa"
      },
      "source": [
        "def train(model_file=BASE_MODEL, batch_size=BATCH_SIZE,\r\n",
        "          warmup_epochs=WARMUP_EPOCHS, train_epochs=TRAIN_EPOCHS,\r\n",
        "          last_epoch=LAST_EPOCH, cloze_test=True):\r\n",
        "    \r\n",
        "    tokenizer = BertTokenizer.from_pretrained(BASE_MODEL)\r\n",
        "    model = BertForNextSentencePrediction.from_pretrained(model_file)\r\n",
        "\r\n",
        "    #Send to GPU and allow Training\r\n",
        "    model = model.to(device)\r\n",
        "    model.train()\r\n",
        "\r\n",
        "    if cloze_test:\r\n",
        "        trainloader = torch.utils.data.DataLoader(ClozeTest(dev=False),\r\n",
        "                                                  batch_size=batch_size,\r\n",
        "                                                  shuffle=True)\r\n",
        "    else:\r\n",
        "        trainloader = torch.utils.data.DataLoader(RocStories(),\r\n",
        "                                                  batch_size=batch_size,\r\n",
        "                                                  shuffle=True)\r\n",
        "\r\n",
        "    #LR maybe needs to be optimized\r\n",
        "    optimizer = AdamW(model.parameters(), lr=1e-5)\r\n",
        "    n_batches =  len(trainloader)\r\n",
        "    scheduler = get_linear_schedule_with_warmup(\r\n",
        "        optimizer,\r\n",
        "        num_warmup_steps=(warmup_epochs * n_batches),\r\n",
        "        num_training_steps=(train_epochs * n_batches),\r\n",
        "        last_epoch=(last_epoch * n_batches)\r\n",
        "    )\r\n",
        "\r\n",
        "    for epoch in tqdm(range(train_epochs)):\r\n",
        "        \r\n",
        "        for stories, labels in trainloader:\r\n",
        "            # this is PyTorch-specific as gradients get accumulated        \r\n",
        "            optimizer.zero_grad()\r\n",
        "\r\n",
        "            start = stories[0]\r\n",
        "            end = stories[1]\r\n",
        "\r\n",
        "            labels = labels.to(device)\r\n",
        "           \r\n",
        "            # Tokenize sentence pairs.\r\n",
        "            # All sequences in batch processing must be same length.\r\n",
        "            # Therefore we use padding to fill shorter sequences\r\n",
        "            # with uninterpreted [PAD] tokens)\r\n",
        "            tokenized_batch = tokenizer(start, padding = True, text_pair = end,\r\n",
        "                                        return_tensors='pt').to(device)\r\n",
        "            \r\n",
        "            outputs = model(**tokenized_batch, labels = labels)           \r\n",
        "            outputs.loss.backward()\r\n",
        "            optimizer.step()\r\n",
        "            scheduler.step() # Huggingface specific: step = epoch\r\n",
        "\r\n",
        "        if cloze_test:\r\n",
        "            model.save_pretrained(currentDirectory + \r\n",
        "                                  \"bertfornsp_finetuned\" + str(epoch))\r\n",
        "        else:\r\n",
        "            model.save_pretrained(currentDirectory +\r\n",
        "                                  \"bertfornsp_roc_finetuned\" + str(epoch))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypcXSNFzu0Ds"
      },
      "source": [
        "def test(model_file=BASE_MODEL, verbose = False):\r\n",
        "    softmax = torch.nn.Softmax(dim=1)\r\n",
        "    tokenizer = BertTokenizer.from_pretrained(BASE_MODEL)\r\n",
        "    model = BertForNextSentencePrediction.from_pretrained(model_file)\r\n",
        "\r\n",
        "    #Send to GPU and allow Evaluation\r\n",
        "    model = model.to(device)\r\n",
        "    model.eval()\r\n",
        "\r\n",
        "    #Dataloader\r\n",
        "    devloader = torch.utils.data.DataLoader(ClozeTest(), batch_size=10)\r\n",
        "\r\n",
        "    pred_list, label_list = list(), list()\r\n",
        "\r\n",
        "    for stories, labels in tqdm(devloader, disable=verbose):\r\n",
        "        \r\n",
        "        start = stories[0]\r\n",
        "        end = stories[1]\r\n",
        "        \r\n",
        "         # Tokenize sentence pairs.\r\n",
        "            # All sequences in batch processing must be same length.\r\n",
        "            # Therefore we use padding to fill shorter sequences\r\n",
        "            # with uninterpreted [PAD] tokens)\r\n",
        "        tokenized_batch = tokenizer(start, padding = True, text_pair = end,\r\n",
        "                                    return_tensors='pt').to(device)\r\n",
        "\r\n",
        "        #Send to GPU\r\n",
        "        labels = labels.to(device)\r\n",
        "\r\n",
        "        outputs = model(**tokenized_batch, labels = labels)\r\n",
        "        logits = outputs.logits\r\n",
        "\r\n",
        "        # Model predicts sentence-pair as correct if True-logit > False-logit\r\n",
        "        predictions = logits.argmax(dim=1).int()\r\n",
        "        probs = softmax(logits).cpu().detach()\r\n",
        "\r\n",
        "        # iterate over elements in batch\r\n",
        "        for i, element_input_ids in enumerate(tokenized_batch.input_ids):\r\n",
        "            \r\n",
        "            # Extra info print() if verbose\r\n",
        "            if verbose:\r\n",
        "                print(tokenizer.decode(element_input_ids))\r\n",
        "                print(\"Probability:\", probs[i][0].item() * 100)\r\n",
        "                print(\"Predicted: \", bool(predictions[i]))\r\n",
        "                print(\"True label: \", bool(labels[i]))\r\n",
        "\r\n",
        "        pred_list.extend(predictions.tolist())\r\n",
        "        label_list.extend(labels.tolist())\r\n",
        "\r\n",
        "    print(confusion_matrix(label_list, pred_list))\r\n",
        "    print(classification_report(label_list, pred_list))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqGaLuNIu4Vk",
        "outputId": "5b8bc293-d5b5-444e-af7d-732df82a542c"
      },
      "source": [
        "if __name__ == \"__main__\":\r\n",
        "    #Test pretrained model\r\n",
        "    test()\r\n",
        "\r\n",
        "    #Fine-tune model \"bertfornsp_finetuned\"\r\n",
        "    #train(cloze_test=True)\r\n",
        "\r\n",
        "    #Test model \"bertfornsp_finetuned\"\r\n",
        "    #test(model_file=\"bertfornsp_finetuned\")\r\n",
        "\r\n",
        "    #Fine-tune model \"bertfornsp_roc_finetuned\"\r\n",
        "    train(cloze_test=False)\r\n",
        "\r\n",
        "    #Test model \"bertfornsp_finetuned\"\r\n",
        "    test(model_file=(currentDirectory + \"bertfornsp_roc_finetuned\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForNextSentencePrediction: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "100%|██████████| 11/11 [29:05<00:00, 158.67s/it]\n",
            "100%|██████████| 375/375 [00:52<00:00,  7.09it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[1578  293]\n",
            " [ 353 1520]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.84      0.83      1871\n",
            "           1       0.84      0.81      0.82      1873\n",
            "\n",
            "    accuracy                           0.83      3744\n",
            "   macro avg       0.83      0.83      0.83      3744\n",
            "weighted avg       0.83      0.83      0.83      3744\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}