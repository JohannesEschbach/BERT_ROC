{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "runBertForNextSentencePrediction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "taj1zzZAvY1U",
        "outputId": "8ad6199c-2c80-46bc-bcaa-329a5f174889"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f9/54/5ca07ec9569d2f232f3166de5457b63943882f7950ddfcc887732fc7fb23/transformers-4.3.3-py3-none-any.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 8.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 42.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/23/2ddc317b2121117bf34dd00f5b0de194158f2a44ee2bf5e47c7166878a97/tokenizers-0.10.1-cp37-cp37m-manylinux2010_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 51.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp37-none-any.whl size=893262 sha256=aa155f950d15763e2b44e0b4a9c979976cecea6bb878fb9da1a894ae9593a406\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.10.1 transformers-4.3.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H2vOFbYWdhth",
        "outputId": "e0e2365a-db08-4341-b46c-7a2d8e87321e"
      },
      "source": [
        "from google.colab import drive\r\n",
        "\r\n",
        "current_directory = '/content/drive/My Drive/FSem/'\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4MUY1yyup2m"
      },
      "source": [
        "import csv\r\n",
        "import torch\r\n",
        "from torch.nn.functional import softmax\r\n",
        "from transformers import BertForNextSentencePrediction, BertTokenizer\r\n",
        "from sklearn.metrics import confusion_matrix\r\n",
        "from sklearn.metrics import classification_report\r\n",
        "from tqdm import tqdm\r\n",
        "from transformers import AdamW\r\n",
        "from transformers import get_linear_schedule_with_warmup\r\n",
        "from matplotlib import pyplot as plt\r\n",
        "\r\n",
        "\r\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\r\n",
        "\r\n",
        "# underlying pretrained LM\r\n",
        "BASE_MODEL = 'bert-large-uncased-whole-word-masking'\r\n",
        "\r\n",
        "BATCH_SIZE = 12\r\n",
        "WARMUP_EPOCHS = 1\r\n",
        "TRAIN_EPOCHS = 10\r\n",
        "LAST_EPOCH = -1"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dw0X81uWJVwd"
      },
      "source": [
        "def getModelFileName(model_name, last_epoch):\r\n",
        "    return current_directory + model_name + str(last_epoch)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdWgHZQtxHi8"
      },
      "source": [
        "class RocStories(torch.utils.data.Dataset):\n",
        "    def __init__(self):    \n",
        "        dataset = []       \n",
        "        with open(current_directory + 'roc_stories.csv', \n",
        "                  'r', encoding='utf-8') as d:\n",
        "            \n",
        "            reader = csv.reader(d, quotechar='\"', delimiter=',',\n",
        "                                quoting=csv.QUOTE_ALL, skipinitialspace=True)                \n",
        "            for line in reader:\n",
        "                dataset.append(line)  \n",
        "\n",
        "        self.data = []\n",
        "        self.labels = []\n",
        "\n",
        "        stories = []\n",
        "        endings = []\n",
        "        for sample in dataset:           \n",
        "            start = \" \".join(sample[2:-1])\n",
        "            stories.append(start)            \n",
        "            end = sample[-1]                        \n",
        "            endings.append(end)\n",
        "\n",
        "        from random import shuffle\n",
        "        wrong_endings = endings.copy()\n",
        "        shuffle(wrong_endings)\n",
        "\n",
        "        assert len(stories) == len(endings)\n",
        "        for i, story in enumerate(stories):\n",
        "            \n",
        "            #True Ending\n",
        "            self.data.append([story, endings[i]])\n",
        "            self.labels.append(0)\n",
        "\n",
        "            #Wrong Ending\n",
        "            self.data.append([story, wrong_endings[i]])\n",
        "            self.labels.append(1)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        X = self.data[idx]\n",
        "        y = self.labels[idx]        \n",
        "        return X, y\n",
        "\n",
        "    def __len__(self):\n",
        "        assert len(self.data) == len(self.labels)\n",
        "        return len(self.labels)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6zAL8cquyF1"
      },
      "source": [
        "class ClozeTest(torch.utils.data.Dataset):\r\n",
        "    def __init__(self, dev=True):\r\n",
        "        \r\n",
        "        dataset = []\r\n",
        "\r\n",
        "        # if dev=True, we load the dev set for testing\r\n",
        "        with open(current_directory + 'cloze_test.csv' if dev else \r\n",
        "                  current_directory + 'cloze_train.csv', \r\n",
        "                  'r', encoding='utf-8') as d:\r\n",
        "            reader = csv.reader(d, quotechar='\"', delimiter=',', \r\n",
        "                                quoting=csv.QUOTE_ALL, skipinitialspace=True)                \r\n",
        "            for line in reader:\r\n",
        "                dataset.append(line) \r\n",
        "\r\n",
        "        self.data = []\r\n",
        "        self.labels = []\r\n",
        "\r\n",
        "        for sample in dataset:\r\n",
        "            \r\n",
        "            start = \" \".join(sample[1:-3])\r\n",
        "            end1 = sample[-3]\r\n",
        "            end2 = sample[-2]\r\n",
        "            right_ending = sample[-1]\r\n",
        "\r\n",
        "            self.data.append([start, end1])\r\n",
        "            self.labels.append(0 if \"1\" == right_ending else 1)\r\n",
        "\r\n",
        "            self.data.append([start, end2])\r\n",
        "            self.labels.append(0 if \"2\" == right_ending else 1)\r\n",
        "\r\n",
        "    def __getitem__(self, idx):\r\n",
        "        X = self.data[idx]\r\n",
        "        y = self.labels[idx]        \r\n",
        "        return X, y\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        assert len(self.data) == len(self.labels)\r\n",
        "        return len(self.labels)\r\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgItyJtueXHR"
      },
      "source": [
        "def weight_diff(model1, model2):\r\n",
        "    pass"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yCLlZ7UPpfa"
      },
      "source": [
        "def train(model_file=BASE_MODEL, batch_size=BATCH_SIZE,\r\n",
        "          warmup_epochs=WARMUP_EPOCHS, train_epochs=TRAIN_EPOCHS,\r\n",
        "          last_epoch=LAST_EPOCH, cloze_test=True, verbose=False):\r\n",
        "    \r\n",
        "    tokenizer = BertTokenizer.from_pretrained(BASE_MODEL)\r\n",
        "    model = BertForNextSentencePrediction.from_pretrained(model_file)\r\n",
        "    # The old weights are saved in model_old to be used to compare to model\r\n",
        "    model_old = BertForNextSentencePrediction.from_pretrained(model_file)\r\n",
        "\r\n",
        "    #Send to GPU and allow Training\r\n",
        "    model = model.to(device)\r\n",
        "    model.train()\r\n",
        "\r\n",
        "    trainloader = torch.utils.data.DataLoader(\r\n",
        "        ClozeTest(dev=False) if cloze_test else RocStories,\r\n",
        "        batch_size=batch_size, shuffle=True\r\n",
        "    )\r\n",
        "\r\n",
        "    #LR maybe needs to be optimized\r\n",
        "    optimizer = AdamW(model.parameters(), lr=1e-5)\r\n",
        "    n_batches =  len(trainloader)\r\n",
        "    scheduler = get_linear_schedule_with_warmup(\r\n",
        "        optimizer,\r\n",
        "        num_warmup_steps=(warmup_epochs * n_batches),\r\n",
        "        num_training_steps=(train_epochs * n_batches),\r\n",
        "        last_epoch=max(-1, last_epoch * n_batches) # actually, last_step\r\n",
        "    )\r\n",
        "    losses = []\r\n",
        "\r\n",
        "    epochs_range = range(last_epoch + 1, train_epochs)\r\n",
        "    for epoch in tqdm(epochs_range):\r\n",
        "        \r\n",
        "        for batchId, (stories, labels) in zip(range(n_batches), trainloader):\r\n",
        "            # this is PyTorch-specific as gradients get accumulated        \r\n",
        "            optimizer.zero_grad()\r\n",
        "\r\n",
        "            start = stories[0]\r\n",
        "            end = stories[1]\r\n",
        "\r\n",
        "            labels = labels.to(device)\r\n",
        "           \r\n",
        "            # Tokenize sentence pairs.\r\n",
        "            # All sequences in batch processing must be same length.\r\n",
        "            # Therefore we use padding to fill shorter sequences\r\n",
        "            # with uninterpreted [PAD] tokens)\r\n",
        "            tokenized_batch = tokenizer(start, padding = True, text_pair = end,\r\n",
        "                                        return_tensors='pt').to(device)\r\n",
        "            \r\n",
        "            loss = model(**tokenized_batch, labels = labels).loss\r\n",
        "            if verbose:\r\n",
        "                print(\"Epoch \" + str(epoch + 1) + \r\n",
        "                      \" Batch \" + batchId + \" of \" + n_batches + \r\n",
        "                      \" Loss: \" + loss.item())\r\n",
        "            losses.append(loss.item())\r\n",
        "\r\n",
        "            loss.backward()\r\n",
        "            optimizer.step()\r\n",
        "            scheduler.step() # Huggingface specific: step = epoch\r\n",
        "\r\n",
        "        model.save_pretrained(\r\n",
        "            getModelFileName((\"bertfornsp_finetuned\" if cloze_test \r\n",
        "                              else \"bertfornsp_roc_finetuned\"), epoch + 1)\r\n",
        "        )\r\n",
        "    \r\n",
        "    # Loss function change over steps is plotted below.\r\n",
        "    plt.plot(losses)\r\n",
        "    plt.xticks(\r\n",
        "        ticks=[(i - last_epoch - 1) * n_batches for i in epochs_range],\r\n",
        "        labels=epochs_range\r\n",
        "    )\r\n",
        "    plt.title((\"Story Cloze\" if cloze_test else \"ROCStories\") + \" Training\")\r\n",
        "    plt.xlabel(\"Epoch\")\r\n",
        "    plt.ylabel(\"Loss\")\r\n",
        "    plt.show()\r\n",
        "\r\n",
        "    # Models are compared\r\n",
        "    weight_diff(model, model_old)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypcXSNFzu0Ds"
      },
      "source": [
        "def test(model_file=BASE_MODEL, verbose = False):\r\n",
        "    softmax = torch.nn.Softmax(dim=1)\r\n",
        "    tokenizer = BertTokenizer.from_pretrained(BASE_MODEL)\r\n",
        "    model = BertForNextSentencePrediction.from_pretrained(model_file)\r\n",
        "\r\n",
        "    #Send to GPU and allow Evaluation\r\n",
        "    model = model.to(device)\r\n",
        "    model.eval()\r\n",
        "\r\n",
        "    #Dataloader\r\n",
        "    devloader = torch.utils.data.DataLoader(ClozeTest(), batch_size=10)\r\n",
        "\r\n",
        "    pred_list, label_list = list(), list()\r\n",
        "\r\n",
        "    for stories, labels in tqdm(devloader, disable=verbose):\r\n",
        "        \r\n",
        "        start = stories[0]\r\n",
        "        end = stories[1]\r\n",
        "        \r\n",
        "        # Tokenize sentence pairs.\r\n",
        "        # All sequences in batch processing must be same length.\r\n",
        "        # Therefore we use padding to fill shorter sequences\r\n",
        "        # with uninterpreted [PAD] tokens)\r\n",
        "        tokenized_batch = tokenizer(start, padding = True, text_pair = end,\r\n",
        "                                    return_tensors='pt').to(device)\r\n",
        "\r\n",
        "        #Send to GPU\r\n",
        "        labels = labels.to(device)\r\n",
        "\r\n",
        "        outputs = model(**tokenized_batch, labels = labels)\r\n",
        "        logits = outputs.logits\r\n",
        "\r\n",
        "        # Model predicts sentence-pair as correct if True-logit > False-logit\r\n",
        "        predictions = logits.argmax(dim=1).int()\r\n",
        "        probs = softmax(logits).cpu().detach()\r\n",
        "\r\n",
        "        # Extra info print() if verbose\r\n",
        "        if verbose:\r\n",
        "            # iterate over elements in batch\r\n",
        "            for i, element_input_ids in enumerate(tokenized_batch.input_ids):\r\n",
        "                print(tokenizer.decode(element_input_ids))\r\n",
        "                print(\"Probability:\", probs[i][0].item() * 100)\r\n",
        "                print(\"Predicted: \", bool(predictions[i]))\r\n",
        "                print(\"True label: \", bool(labels[i]))\r\n",
        "\r\n",
        "        pred_list.extend(predictions.tolist())\r\n",
        "        label_list.extend(labels.tolist())\r\n",
        "\r\n",
        "    print(confusion_matrix(label_list, pred_list))\r\n",
        "    print(classification_report(label_list, pred_list))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SqGaLuNIu4Vk",
        "outputId": "a05a1027-3caf-4371-a675-3bd0d7bb728f"
      },
      "source": [
        "if __name__ == \"__main__\":\r\n",
        "    #Test pretrained model\r\n",
        "    test()\r\n",
        "\r\n",
        "    #Fine-tune model \"bertfornsp_finetuned\"\r\n",
        "    train(cloze_test=True)\r\n",
        "\r\n",
        "    #Test model \"bertfornsp_finetuned\"\r\n",
        "    test(model_file=getModelFileName(\"bertfornsp_finetuned\", TRAIN_EPOCHS))\r\n",
        "\r\n",
        "    #Fine-tune model \"bertfornsp_roc_finetuned\"\r\n",
        "    train(model_file=getModelFileName(\"bertfornsp_finetuned\", TRAIN_EPOCHS),\r\n",
        "          train_epochs=1, cloze_test=False)\r\n",
        "\r\n",
        "    #Test model \"bertfornsp_finetuned\"\r\n",
        "    test(model_file=getModelFileName(\"bertfornsp_roc_finetuned\", 1))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForNextSentencePrediction: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "100%|██████████| 375/375 [00:50<00:00,  7.41it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[ 348 1523]\n",
            " [ 298 1575]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.54      0.19      0.28      1871\n",
            "           1       0.51      0.84      0.63      1873\n",
            "\n",
            "    accuracy                           0.51      3744\n",
            "   macro avg       0.52      0.51      0.46      3744\n",
            "weighted avg       0.52      0.51      0.46      3744\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForNextSentencePrediction: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForNextSentencePrediction: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "100%|██████████| 10/10 [28:24<00:00, 170.49s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "404 Client Error: Not Found for url: https://huggingface.co//content/drive/My%20Drive/FSem/bertfornsp_finetuned10/resolve/main/config.json\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    423\u001b[0m                 \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m                 \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m   1085\u001b[0m             \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1086\u001b[0;31m             \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1087\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0metag_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m             \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1217\u001b[0m             \u001b[0metag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"X-Linked-Etag\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ETag\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    940\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 941\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    942\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://huggingface.co//content/drive/My%20Drive/FSem/bertfornsp_finetuned10/resolve/main/config.json",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-3c11fee8ac08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m#Test model \"bertfornsp_finetuned\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgetModelFileName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bertfornsp_finetuned\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTRAIN_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m#Fine-tune model \"bertfornsp_roc_finetuned\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-b523c20f8059>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(model_file, verbose)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0msoftmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBASE_MODEL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertForNextSentencePrediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m#Send to GPU and allow Evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    960\u001b[0m                 \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m                 \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 962\u001b[0;31m                 \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    963\u001b[0m             )\n\u001b[1;32m    964\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \"\"\"\n\u001b[0;32m--> 376\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    434\u001b[0m                 \u001b[0;34mf\"- or '{pretrained_model_name_or_path}' is the correct path to a directory containing a {CONFIG_NAME} file\\n\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m             )\n\u001b[0;32m--> 436\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: Can't load config for '/content/drive/My Drive/FSem/bertfornsp_finetuned10'. Make sure that:\n\n- '/content/drive/My Drive/FSem/bertfornsp_finetuned10' is a correct model identifier listed on 'https://huggingface.co/models'\n\n- or '/content/drive/My Drive/FSem/bertfornsp_finetuned10' is the correct path to a directory containing a config.json file\n\n"
          ]
        }
      ]
    }
  ]
}