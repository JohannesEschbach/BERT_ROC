{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"StoriesSemantics.ipynb","provenance":[{"file_id":"https://github.com/JohannesEschbach/BERT_ROC/blob/johannes/runBertForNextSentencePrediction.ipynb","timestamp":1615753750868}],"collapsed_sections":["VGUHTlObOfPr","EJO2HeysOmYx","2EDcfMlhItBd","x1-E16-F984x","YROdTgL0IM3U","8CkrBmqpBUQi","8faj36chTaL_"],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"48bc834a572b4caea553ebdf503f2805":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_47015140fe974c8b93c4892c21f8443a","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_04e3744ceec747e0991dd1b983e9c182","IPY_MODEL_aec79fd6a7db4eeab3da94a9b9deaee1"]}},"47015140fe974c8b93c4892c21f8443a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"04e3744ceec747e0991dd1b983e9c182":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_e50a9eeaa40f44489e836fecde3f9488","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":231508,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":231508,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5058110ecc7f4c8aa4503ea66e4b84f3"}},"aec79fd6a7db4eeab3da94a9b9deaee1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_b0ad911e432742fcb4d1f07f04e164cd","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 232k/232k [00:00&lt;00:00, 741kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_36e5bde718294941a5af2777e868c15d"}},"e50a9eeaa40f44489e836fecde3f9488":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"5058110ecc7f4c8aa4503ea66e4b84f3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b0ad911e432742fcb4d1f07f04e164cd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"36e5bde718294941a5af2777e868c15d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0e7419e89da14f56aa4b139f0b166aa8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_0e2eb3fa60b24fec958cf9a999d7b0de","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_e508a5b6815e4c24bdcf4159ed0a1cbc","IPY_MODEL_fd0742d6d3654dd2a2c9cc41a0e14c3f"]}},"0e2eb3fa60b24fec958cf9a999d7b0de":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e508a5b6815e4c24bdcf4159ed0a1cbc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_139af509727244b590e605cb5ff3072b","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":28,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":28,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_4648acf5c5f446509474d055371d2885"}},"fd0742d6d3654dd2a2c9cc41a0e14c3f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_f1b9a86fe0d845a08304399f63d8f2d3","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 28.0/28.0 [00:00&lt;00:00, 159B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_20caba27808345d48e64ffafac8af693"}},"139af509727244b590e605cb5ff3072b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"4648acf5c5f446509474d055371d2885":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f1b9a86fe0d845a08304399f63d8f2d3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"20caba27808345d48e64ffafac8af693":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"92254b324ed64720aeafc20406227b52":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_0a53f074e27d4cd685ede72dc713f195","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_0e936f0da5184904b98cbe53453be9cd","IPY_MODEL_19315383b0a74f7caaf801f12302b1eb"]}},"0a53f074e27d4cd685ede72dc713f195":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0e936f0da5184904b98cbe53453be9cd":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_a62c79e1b8114feebc28639702988e5d","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":466062,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":466062,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e1a628c8050a4eb8b4bda16af0d377d9"}},"19315383b0a74f7caaf801f12302b1eb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_5e0391ae83b74f7bb033ea241dce1dcd","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 466k/466k [00:00&lt;00:00, 5.37MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_20840f2e8dc5442ca3da6ce181648c61"}},"a62c79e1b8114feebc28639702988e5d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"e1a628c8050a4eb8b4bda16af0d377d9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5e0391ae83b74f7bb033ea241dce1dcd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"20840f2e8dc5442ca3da6ce181648c61":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"43f11a89945548299b1a1fe70cdd02b8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_9a66b09c8fac4847b8fe6dd9264063b3","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_bcfb4225c10a4584914f48bb464fe673","IPY_MODEL_eb58485470ef458c9467b27feff065f1"]}},"9a66b09c8fac4847b8fe6dd9264063b3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"bcfb4225c10a4584914f48bb464fe673":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_fcce2d5ba4464f7e819abaf14983b0c0","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":434,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":434,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e48d9030d6884af78970b1f76b6363d4"}},"eb58485470ef458c9467b27feff065f1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_3084e1527db24e6a88197360d2c658a1","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 434/434 [00:00&lt;00:00, 4.24kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3f9f2cd1abb54d5cbf72f888b101f395"}},"fcce2d5ba4464f7e819abaf14983b0c0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"e48d9030d6884af78970b1f76b6363d4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3084e1527db24e6a88197360d2c658a1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"3f9f2cd1abb54d5cbf72f888b101f395":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c53c30ca6eb549cebf68975752ed7df9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_2d83a898e1bf4f948bf5129e4ebea6de","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_dbc603d6b2ae42a5adcece4d6c2ad88a","IPY_MODEL_249c5f6ddc9e41a2972e27b4e67e8aac"]}},"2d83a898e1bf4f948bf5129e4ebea6de":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"dbc603d6b2ae42a5adcece4d6c2ad88a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_dfb189e448634b779d5371273238b634","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":1345000548,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1345000548,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5f88b0edc3c94a5fbf579d42846963f1"}},"249c5f6ddc9e41a2972e27b4e67e8aac":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_a4036525e2ec405ab2df8086fbe11db0","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1.35G/1.35G [00:26&lt;00:00, 51.2MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7fac0e91299d410aaf340e84c8949912"}},"dfb189e448634b779d5371273238b634":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"5f88b0edc3c94a5fbf579d42846963f1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a4036525e2ec405ab2df8086fbe11db0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"7fac0e91299d410aaf340e84c8949912":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"taj1zzZAvY1U","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617103747496,"user_tz":-120,"elapsed":6919,"user":{"displayName":"Johannes Eschbach","photoUrl":"","userId":"07016017118869064956"}},"outputId":"e5719a32-2f70-4b5e-b43c-ab429eccfc64"},"source":["!pip install transformers"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/d5/f4157a376b8a79489a76ce6cfe147f4f3be1e029b7144fa7b8432e8acb26/transformers-4.4.2-py3-none-any.whl (2.0MB)\n","\u001b[K     |████████████████████████████████| 2.0MB 18.5MB/s \n","\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/23/2ddc317b2121117bf34dd00f5b0de194158f2a44ee2bf5e47c7166878a97/tokenizers-0.10.1-cp37-cp37m-manylinux2010_x86_64.whl (3.2MB)\n","\u001b[K     |████████████████████████████████| 3.2MB 52.5MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 55.9MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp37-none-any.whl size=893262 sha256=84b70fbe728790981ab87ddaf1602d5eb010dd64459305b63ad1afff318e8147\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: tokenizers, sacremoses, transformers\n","Successfully installed sacremoses-0.0.43 tokenizers-0.10.1 transformers-4.4.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H2vOFbYWdhth","executionInfo":{"status":"ok","timestamp":1617103765646,"user_tz":-120,"elapsed":25061,"user":{"displayName":"Johannes Eschbach","photoUrl":"","userId":"07016017118869064956"}},"outputId":"7a08aeb3-5a5a-4c51-8a0e-741c588b246f"},"source":["from google.colab import drive\n"," \n","current_directory = '/content/drive/My Drive/Semantics/BERT_ROC/'\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Q3bJ_83SCkEM"},"source":["Go sure current directory contains test sets and models\n","\n","Models:\n","\n","ClozeOnly: https://drive.google.com/drive/folders/119WnpHBmM637M0SVk3buy-KPI5aWpsF0?usp=sharing\n","\n","RocOnly: https://drive.google.com/drive/folders/1eVdGH5DVEDCrGMSc9thTwJo18btzXras?usp=sharing\n","\n","Cloze + 5000 Roc: https://drive.google.com/drive/folders/1-XfWuEsxEAKUby35Zz_y9zEo6kyDuSRF?usp=sharing"]},{"cell_type":"markdown","metadata":{"id":"VGUHTlObOfPr"},"source":["# Headers and Global Variables"]},{"cell_type":"code","metadata":{"id":"D4MUY1yyup2m","executionInfo":{"status":"ok","timestamp":1617103773279,"user_tz":-120,"elapsed":6425,"user":{"displayName":"Johannes Eschbach","photoUrl":"","userId":"07016017118869064956"}}},"source":["import csv\n","import torch\n","from torch.nn.functional import softmax\n","from torch.nn.functional import relu\n","from transformers import BertForNextSentencePrediction, BertTokenizer\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import classification_report\n","from tqdm import tqdm\n","from transformers import AdamW\n","from transformers import get_linear_schedule_with_warmup\n","import matplotlib\n","from matplotlib import pyplot as plt\n","from matplotlib import pyplot as plt\n","from IPython.display import display, HTML\n","\n","import os\n","import sys\n","from pathlib import Path\n","\n","project_path = Path(os.path.dirname(os.path.realpath(sys.argv[0]))).parent\n","modelpath = str(project_path.joinpath('models')) + \"/\"\n","datapath = str(project_path.joinpath('datasets')) + \"/\"\n","\n","device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n","\n","CLOZE_MODEL = 'bertfornsp_clozeonly_finetuned'\n","ROC_MODEL = 'bertfornsp_roc_finetuned'\n","# underlying pretrained LM\n","BASE_MODEL = 'bert-large-uncased-whole-word-masking'\n","\n","BATCH_SIZE = 12\n","WARMUP_EPOCHS = 1\n","TRAIN_EPOCHS = 10\n","LAST_EPOCH = -1"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EJO2HeysOmYx"},"source":["# Datasets"]},{"cell_type":"code","metadata":{"id":"fdWgHZQtxHi8","executionInfo":{"status":"ok","timestamp":1617103773280,"user_tz":-120,"elapsed":4725,"user":{"displayName":"Johannes Eschbach","photoUrl":"","userId":"07016017118869064956"}}},"source":["class RocStories(torch.utils.data.Dataset):\n","    def __init__(self, short = False):    \n","        dataset = []       \n","        with open(datapath + 'roc_stories.csv', \n","                  'r', encoding='utf-8') as d:\n","            \n","            reader = csv.reader(d, quotechar='\"', delimiter=',',\n","                                quoting=csv.QUOTE_ALL, skipinitialspace=True)                \n","            for line in reader:\n","                dataset.append(line)  \n","\n","        self.data = []\n","        self.labels = []\n","\n","        stories = []\n","        endings = []\n","        for i, sample in enumerate(dataset):\n","            if short == True:\n","                if i >= 5000: break           \n","            start = \" \".join(sample[2:-1])\n","            stories.append(start)            \n","            end = sample[-1]                        \n","            endings.append(end)\n","\n","        from random import shuffle\n","        wrong_endings = endings.copy()\n","        shuffle(wrong_endings)\n","\n","        assert len(stories) == len(endings)\n","        for i, story in enumerate(stories):\n","            \n","            #True Ending\n","            self.data.append([story, endings[i]])\n","            self.labels.append(0)\n","\n","            #Wrong Ending\n","            self.data.append([story, wrong_endings[i]])\n","            self.labels.append(1)\n","\n","    def __getitem__(self, idx):\n","        X = self.data[idx]\n","        y = self.labels[idx]        \n","        return X, y\n","\n","    def __len__(self):\n","        assert len(self.data) == len(self.labels)\n","        return len(self.labels)"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"N6zAL8cquyF1","executionInfo":{"status":"ok","timestamp":1617103773281,"user_tz":-120,"elapsed":4724,"user":{"displayName":"Johannes Eschbach","photoUrl":"","userId":"07016017118869064956"}}},"source":["class ClozeTest(torch.utils.data.Dataset):\n","    def __init__(self, dev=True, hypothesis_only=False, file = None):\n","        \"\"\"\n","        :param hypothesis_only: Replaces story with empty string. Only Keeps endings as they are.\n","        :param hard: For future hard_test_set.csv\n","        \"\"\"\n","\n","        dataset = []\n","\n","        # if dev=True, we load the dev set for testing\n","        dir = \"\"\n","        \n","        if file is None:\n","          if dev:\n","              dir = datapath + 'cloze_test.csv'\n","          else:\n","              dir = datapath + 'cloze_train.csv'\n","        else: dir = datapath + file\n","\n","        with open(dir, 'r', encoding='utf-8') as d:\n","            reader = csv.reader(d, quotechar='\"', delimiter=',', \n","                                quoting=csv.QUOTE_ALL, skipinitialspace=True)                \n","            for line in reader:\n","                dataset.append(line) \n","            dataset.pop(0)\n","\n","        self.data = []\n","        self.labels = []\n","\n","        for sample in dataset:\n","            \n","            start = \" \".join(sample[1:-3])\n","            if hypothesis_only: start = \"\"\n","            end1 = sample[-3]\n","            end2 = sample[-2]\n","            right_ending = sample[-1]\n","\n","            self.data.append([start, end1])\n","            self.labels.append(0 if \"1\" == right_ending else 1)\n","\n","            self.data.append([start, end2])\n","            self.labels.append(0 if \"2\" == right_ending else 1)\n","\n","    def __getitem__(self, idx):\n","        X = self.data[idx]\n","        y = self.labels[idx]        \n","        return X, y\n","\n","    def __len__(self):\n","        assert len(self.data) == len(self.labels)\n","        return len(self.labels)"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"JyVAy1RKYMTR","executionInfo":{"status":"ok","timestamp":1617103773282,"user_tz":-120,"elapsed":4723,"user":{"displayName":"Johannes Eschbach","photoUrl":"","userId":"07016017118869064956"}}},"source":["class ClozeTest_MC(torch.utils.data.Dataset):\n","    def __init__(self, dev=True,  hypothesis_only=False, file = None):\n","        \n","        dataset = []\n"," \n","        dir = \"\"\n","        \n","        if file is None:\n","          if dev:\n","              dir = datapath + 'cloze_test.csv'\n","          else:\n","              dir = datapath + 'cloze_train.csv'\n","        else: dir = datapath + file\n","\n","        # if dev=True, we load the dev set for testing\n","        with open(dir, 'r', encoding='utf-8') as d:\n","            reader = csv.reader(d, quotechar='\"', delimiter=',', \n","                                quoting=csv.QUOTE_ALL, skipinitialspace=True)                \n","            for line in reader:\n","                dataset.append(line) \n","            dataset.pop(0)\n"," \n","        self.data = []\n","        self.labels = []\n"," \n","        for sample in dataset:\n","            \n","            start = \" \".join(sample[1:-3])\n","            if hypothesis_only: start = \"\"\n","            end1 = sample[-3]\n","            end2 = sample[-2]\n","            right_ending = sample[-1]\n"," \n","            self.data.append([start, end1, end2])\n","            self.labels.append(0 if \"1\" == right_ending else 1)\n"," \n","    def __getitem__(self, idx):\n","        X = self.data[idx]\n","        y = self.labels[idx]        \n","        return X, y\n"," \n","    def __len__(self):\n","        assert len(self.data) == len(self.labels)\n","        return len(self.labels)"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l5vXXr3pOpwh"},"source":["# Auxiliary Functions"]},{"cell_type":"code","metadata":{"id":"dw0X81uWJVwd","executionInfo":{"status":"ok","timestamp":1617103774701,"user_tz":-120,"elapsed":744,"user":{"displayName":"Johannes Eschbach","photoUrl":"","userId":"07016017118869064956"}}},"source":["def getModelFileName(model_name, last_epoch):\n","    return modelpath + model_name + str(last_epoch)"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"BgItyJtueXHR","executionInfo":{"status":"ok","timestamp":1617103774701,"user_tz":-120,"elapsed":744,"user":{"displayName":"Johannes Eschbach","photoUrl":"","userId":"07016017118869064956"}}},"source":["def weight_diff(model1, model2):\n","    diff = torch.nn.MSELoss() # diff(a, b) = ((a - b) ** 2).mean()\n","\n","    xweights, yweights, xbiases, ybiases = dict(), dict(), dict(), dict()\n","    layer_names = set()\n","\n","    for (name, parameter1), parameter2 in zip(\n","        model1.bert.encoder.layer.named_parameters(),\n","        model2.bert.encoder.layer.parameters()\n","    ):\n","\n","        difference = diff(parameter1, parameter2).item()\n","\n","        name = name.split(\".\")\n","        xtick = float(name[0])\n","        layer_name = \".\".join(name[1:-1])\n","        parameter_type = name[-1]\n","\n","        if layer_name not in layer_names:\n","            layer_names.add(layer_name)\n","            xweights[layer_name], xbiases[layer_name] = list(), list()\n","            yweights[layer_name], ybiases[layer_name] = list(), list()\n","\n","        if parameter_type == \"weight\":\n","            yweights[layer_name].append(difference)\n","            xweights[layer_name].append(xtick + 0.0)\n","        else: # if parameter_type == \"bias\"\n","            ybiases[layer_name].append(difference)\n","            xbiases[layer_name].append(xtick + 0.5)\n","\n","    for name in layer_names:\n","        plt.bar(xweights[name], yweights[name], width=0.4, label=\"weight\")\n","        plt.bar(xbiases[name], ybiases[name], width=0.4, label=\"bias\")\n","        plt.xticks(xweights[name])\n","        plt.legend()\n","        plt.title(name)\n","        plt.show()"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PECOGcFZOx5h"},"source":["# Functions for Training and Testing"]},{"cell_type":"code","metadata":{"id":"_yCLlZ7UPpfa","executionInfo":{"status":"ok","timestamp":1617103776518,"user_tz":-120,"elapsed":618,"user":{"displayName":"Johannes Eschbach","photoUrl":"","userId":"07016017118869064956"}}},"source":["def train(cloze_test, model_file=BASE_MODEL, batch_size=BATCH_SIZE,\n","          warmup_epochs=WARMUP_EPOCHS, train_epochs=TRAIN_EPOCHS,\n","          last_epoch=LAST_EPOCH, verbose=False, model_name=None):\n","    \n","    tokenizer = BertTokenizer.from_pretrained(BASE_MODEL)\n","    model = BertForNextSentencePrediction.from_pretrained(model_file)\n","    # The old weights are saved in model_old to be used to compare to model\n","    model_old = BertForNextSentencePrediction.from_pretrained(model_file)\n","\n","    #Send to GPU and allow Training\n","    model = model.to(device)\n","    model.train()\n","\n","    trainloader = torch.utils.data.DataLoader(\n","        ClozeTest(dev=False) if cloze_test else RocStories(),\n","        batch_size=batch_size, shuffle=True\n","    )\n","\n","    #LR maybe needs to be optimized\n","    optimizer = AdamW(model.parameters(), lr=1e-5)\n","    n_batches =  len(trainloader)\n","    scheduler = get_linear_schedule_with_warmup(\n","        optimizer,\n","        num_warmup_steps=(warmup_epochs * n_batches),\n","        num_training_steps=(train_epochs * n_batches),\n","        last_epoch=max(-1, last_epoch * n_batches) # actually, last_step\n","    )\n","    losses = []\n","\n","    epochs_range = range(last_epoch + 1, train_epochs)\n","    for epoch in tqdm(epochs_range):\n","        \n","        for batchId, (stories, labels) in zip(range(n_batches), trainloader):\n","            # this is PyTorch-specific as gradients get accumulated        \n","            optimizer.zero_grad()\n","\n","            start = stories[0]\n","            end = stories[1]\n","\n","            labels = labels.to(device)\n","           \n","            # Tokenize sentence pairs.\n","            # All sequences in batch processing must be same length.\n","            # Therefore we use padding to fill shorter sequences\n","            # with uninterpreted [PAD] tokens)\n","            tokenized_batch = tokenizer(start, padding = True, text_pair = end,\n","                                        return_tensors='pt').to(device)\n","            \n","            loss = model(**tokenized_batch, labels = labels).loss\n","            if verbose:\n","                print(\"Epoch \" + str(epoch + 1) + \n","                      \" Batch \" + batchId + \" of \" + n_batches + \n","                      \" Loss: \" + loss.item())\n","            losses.append(loss.item())\n","\n","            loss.backward()\n","            optimizer.step()\n","            scheduler.step() # Huggingface specific: step = epoch\n","\n","        model.save_pretrained(\n","            getModelFileName(model_name, epoch + 1)\n","        )\n","    \n","    # Loss function change over steps is plotted below.\n","    plt.plot(losses)\n","    plt.xticks(\n","        ticks=[(i - last_epoch - 1) * n_batches for i in epochs_range],\n","        labels=epochs_range\n","    )\n","    plt.title((\"Story Cloze\" if cloze_test else \"ROCStories\") + \" Training\")\n","    plt.xlabel(\"Epoch\")\n","    plt.ylabel(\"Loss\")\n","    plt.show()\n","\n","    # Models are compared\n","    weight_diff(model, model_old)"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"ypcXSNFzu0Ds","executionInfo":{"status":"ok","timestamp":1617103777639,"user_tz":-120,"elapsed":1737,"user":{"displayName":"Johannes Eschbach","photoUrl":"","userId":"07016017118869064956"}}},"source":["def test(model_file=BASE_MODEL, verbose = False, cloze_test = ClozeTest()):\n","    softmax = torch.nn.Softmax(dim=1)\n","    tokenizer = BertTokenizer.from_pretrained(BASE_MODEL)\n","    model = BertForNextSentencePrediction.from_pretrained(model_file)\n","\n","    #Send to GPU and allow Evaluation\n","    model = model.to(device)\n","    model.eval()\n","\n","    #Dataloader\n","    devloader = torch.utils.data.DataLoader(cloze_test, batch_size=10)\n","\n","    pred_list, label_list = list(), list()\n","\n","    for stories, labels in tqdm(devloader, disable=verbose):\n","        \n","        start = stories[0]\n","        end = stories[1]\n","        \n","        # Tokenize sentence pairs.\n","        # All sequences in batch processing must be same length.\n","        # Therefore we use padding to fill shorter sequences\n","        # with uninterpreted [PAD] tokens)\n","        tokenized_batch = tokenizer(start, padding = True, text_pair = end,\n","                                    return_tensors='pt').to(device)\n","\n","        #Send to GPU\n","        labels = labels.to(device)\n","\n","        outputs = model(**tokenized_batch, labels = labels)\n","        logits = outputs.logits\n","\n","        # Model predicts sentence-pair as correct if True-logit > False-logit\n","        predictions = logits.argmax(dim=1).int()\n","        probs = softmax(logits).cpu().detach()\n","\n","        # Extra info print() if verbose\n","        if verbose:\n","            # iterate over elements in batch\n","            for i, element_input_ids in enumerate(tokenized_batch.input_ids):\n","                print(tokenizer.decode(element_input_ids))\n","                print(\"Probability:\", probs[i][0].item() * 100)\n","                print(\"Predicted: \", bool(predictions[i]))\n","                print(\"True label: \", bool(labels[i]))\n","\n","        pred_list.extend(predictions.tolist())\n","        label_list.extend(labels.tolist())\n","\n","    #print(confusion_matrix(label_list, pred_list))\n","    print(classification_report(label_list, pred_list))\n"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"mSMuVmylKNvD","executionInfo":{"status":"ok","timestamp":1617103777936,"user_tz":-120,"elapsed":2032,"user":{"displayName":"Johannes Eschbach","photoUrl":"","userId":"07016017118869064956"}}},"source":["def train_MC(cloze_test = ClozeTest_MC(dev=False), model_file=BASE_MODEL, batch_size=BATCH_SIZE,\n","          warmup_epochs=WARMUP_EPOCHS, train_epochs=TRAIN_EPOCHS,\n","          last_epoch=LAST_EPOCH, verbose=False, model_name=None):\n","    \n","    tokenizer = BertTokenizer.from_pretrained(BASE_MODEL)\n","    model = BertForNextSentencePrediction.from_pretrained(model_file)\n","    # The old weights are saved in model_old to be used to compare to model\n","    model_old = BertForNextSentencePrediction.from_pretrained(model_file)\n","\n","    #Send to GPU and allow Training\n","    model = model.to(device)\n","    model.train()\n","\n","    trainloader = torch.utils.data.DataLoader(cloze_test, batch_size=batch_size, shuffle=True)\n","\n","    #LR maybe needs to be optimized\n","    optimizer = AdamW(model.parameters(), lr=1e-5)\n","    n_batches =  len(trainloader)\n","    scheduler = get_linear_schedule_with_warmup(\n","        optimizer,\n","        num_warmup_steps=(warmup_epochs * n_batches),\n","        num_training_steps=(train_epochs * n_batches),\n","        last_epoch=max(-1, last_epoch * n_batches) # actually, last_step\n","    )\n","    \n","    loss_fct = torch.nn.CrossEntropyLoss()\n","\n","    losses = []\n","    epochs_range = range(last_epoch + 1, train_epochs)\n","    for epoch in tqdm(epochs_range):\n","        \n","        for batchId, (stories, labels) in zip(range(n_batches), trainloader):\n","            # this is PyTorch-specific as gradients get accumulated        \n","            optimizer.zero_grad()\n","\n","            start = stories[0]\n","            end1 = stories[1]\n","            end2 = stories[2]\n","\n","            tokenized_batch_end1 = tokenizer(start, padding = True, text_pair = end1,\n","                                        return_tensors='pt').to(device)\n","            \n","            tokenized_batch_end2 = tokenizer(start, padding = True, text_pair = end2,\n","                                        return_tensors='pt').to(device) \n","    \n","            #Send to GPU\n","            labels = labels.to(device)\n","    \n","            \n","            logits0 = model(**tokenized_batch_end1).logits\n","            logits1 = model(**tokenized_batch_end2).logits    \n","\n","            logits_combined = logits0 + logits1.flip(-1)\n","            loss = loss_fct(logits_combined.view(-1,2), labels.view(-1))\n","            losses.append(loss.item())\n","\n","            \"\"\"\n","            loss = 0            \n","            for i in range(len(labels.data)): #Iterate through batch\n","                log0 = logits0.data[i].to(device)\n","                log1 = logits1.data[i].to(device)\n","                if labels[i].item() == 0:\n","                    label0 = torch.tensor([1,0])\n","                    label1 = torch.tensor([0,1])\n","                else:\n","                    label0 = torch.tensor([0,1])\n","                    label1 = torch.tensor([1,0])\n","\n","                label0 = label0.to(device)\n","                label1 = label1.to(device)\n","\n","                logits_combined = (log0 + log1.flip(-1)) * (label0 + label1.flip(-1))\n","                \n","                loss += loss_fct(logits_combined.unsqueeze(0), labels[i].unsqueeze(0))\n","\n","            \"\"\"\n","\n","            loss.backward()\n","            optimizer.step()\n","            scheduler.step() # Huggingface specific: step = epoch\n","\n","        model.save_pretrained(\n","            getModelFileName(model_name, epoch + 1)\n","        )\n","    \n","    # Loss function change over steps is plotted below.\n","    plt.plot(losses)\n","    plt.xticks(\n","        ticks=[(i - last_epoch - 1) * n_batches for i in epochs_range],\n","        labels=epochs_range\n","    )\n","    plt.title((\"Story Cloze\" if cloze_test else \"ROCStories\") + \" Training\")\n","    plt.xlabel(\"Epoch\")\n","    plt.ylabel(\"Loss\")\n","    plt.show()\n","\n","    # Models are compared\n","    weight_diff(model, model_old)"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"YXZqHmtHZLxC","executionInfo":{"status":"ok","timestamp":1617103778133,"user_tz":-120,"elapsed":2228,"user":{"displayName":"Johannes Eschbach","photoUrl":"","userId":"07016017118869064956"}}},"source":["def test_MC(model_file=BASE_MODEL, verbose = False, cloze_test = ClozeTest_MC(dev=True)):\n","    softmax = torch.nn.Softmax(dim=1)\n","    tokenizer = BertTokenizer.from_pretrained(BASE_MODEL)\n","    model = BertForNextSentencePrediction.from_pretrained(model_file)\n"," \n","    #Send to GPU and allow Evaluation\n","    model = model.to(device)\n","    model.eval()\n"," \n","    #Dataloader\n","    devloader = torch.utils.data.DataLoader(cloze_test, batch_size=10)\n"," \n","    pred_list, label_list = list(), list()\n"," \n","    for stories, labels in tqdm(devloader, disable=verbose):\n","        \n","        start = stories[0]\n","        end1 = stories[1]\n","        end2 = stories[2]\n"," \n","        tokenized_batch_end1 = tokenizer(start, padding = True, text_pair = end1,\n","                                    return_tensors='pt').to(device)\n","        \n","        tokenized_batch_end2 = tokenizer(start, padding = True, text_pair = end2,\n","                                    return_tensors='pt').to(device) \n"," \n","        #Send to GPU\n","        labels = labels.to(device)       \n","        \n","        logits0 = model(**tokenized_batch_end1).logits\n","        logits1 = model(**tokenized_batch_end2).logits    \n","\n","        logits = logits0 + logits1.flip(-1)\n","        \n","        predictions = logits.argmax(dim=1).int()\n","        probs = softmax(logits).cpu().detach()        \n","        \n","\n","        # Extra info print() if verbose\n","        if verbose:\n","            # iterate over elements in batch\n","            for i, element_input_ids in enumerate(tokenized_batch.input_ids):\n","                print(tokenizer.decode(element_input_ids))\n","                print(\"Probability:\", probs[i][0].item() * 100)\n","                print(\"Predicted: \", bool(predictions[i]))\n","                print(\"True label: \", bool(labels[i]))\n"," \n","        pred_list.extend(predictions.tolist())\n","        label_list.extend(labels.tolist())\n"," \n","    #print(confusion_matrix(label_list, pred_list))\n","\n","    print(classification_report(label_list, pred_list))"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"OE13bxsw9zSz"},"source":["def train_mixed(model_file=BASE_MODEL, batch_size=BATCH_SIZE,\n","          warmup_epochs=WARMUP_EPOCHS, train_epochs=TRAIN_EPOCHS,\n","          last_epoch=LAST_EPOCH, verbose=False, model_name=None):\n","    \n","    tokenizer = BertTokenizer.from_pretrained(BASE_MODEL)\n","    model = BertForNextSentencePrediction.from_pretrained(model_file)\n","    # The old weights are saved in model_old to be used to compare to model\n","    model_old = BertForNextSentencePrediction.from_pretrained(model_file)\n","\n","    #Send to GPU and allow Training\n","    model = model.to(device)\n","    model.train()\n","\n","    cloze = ClozeTest(dev=False)\n","    roc = RocStories(short = True)\n","    cloze.data.extend(roc.data)\n","    cloze.labels.extend(roc.labels)\n","\n","\n","    trainloader = torch.utils.data.DataLoader(cloze, batch_size=batch_size, shuffle=True)\n","\n","\n","    #LR maybe needs to be optimized\n","    optimizer = AdamW(model.parameters(), lr=1e-5)\n","    n_batches =  len(trainloader)\n","    scheduler = get_linear_schedule_with_warmup(\n","        optimizer,\n","        num_warmup_steps=(warmup_epochs * n_batches),\n","        num_training_steps=(train_epochs * n_batches),\n","        last_epoch=max(-1, last_epoch * n_batches) # actually, last_step\n","    )\n","    losses = []\n","\n","    epochs_range = range(last_epoch + 1, train_epochs)\n","    for epoch in tqdm(epochs_range):\n","        \n","        for batchId, (stories, labels) in zip(range(n_batches), trainloader):\n","            # this is PyTorch-specific as gradients get accumulated        \n","            optimizer.zero_grad()\n","\n","            start = stories[0]\n","            end = stories[1]\n","\n","            labels = labels.to(device)\n","           \n","            # Tokenize sentence pairs.\n","            # All sequences in batch processing must be same length.\n","            # Therefore we use padding to fill shorter sequences\n","            # with uninterpreted [PAD] tokens)\n","            tokenized_batch = tokenizer(start, padding = True, text_pair = end,\n","                                        return_tensors='pt').to(device)\n","            \n","            loss = model(**tokenized_batch, labels = labels).loss\n","            if verbose:\n","                print(\"Epoch \" + str(epoch + 1) + \n","                      \" Batch \" + batchId + \" of \" + n_batches + \n","                      \" Loss: \" + loss.item())\n","            losses.append(loss.item())\n","\n","            loss.backward()\n","            optimizer.step()\n","            scheduler.step() # Huggingface specific: step = epoch\n","\n","        model.save_pretrained(\n","            getModelFileName(model_name, epoch + 1)\n","        )\n","    \n","    # Loss function change over steps is plotted below.\n","    plt.plot(losses)\n","    plt.xticks(\n","        ticks=[(i - last_epoch - 1) * n_batches for i in epochs_range],\n","        labels=epochs_range\n","    )\n","    plt.title((\"Story Cloze\" if cloze_test else \"ROCStories\") + \" Training\")\n","    plt.xlabel(\"Epoch\")\n","    plt.ylabel(\"Loss\")\n","    plt.show()\n","\n","    # Models are compared\n","    weight_diff(model, model_old)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VwY6i_K19TBz"},"source":["# Training"]},{"cell_type":"markdown","metadata":{"id":"SXgTW9ZGIshN"},"source":["## RocOnly "]},{"cell_type":"code","metadata":{"id":"60O5LnU2IsyG","colab":{"base_uri":"https://localhost:8080/","height":350,"referenced_widgets":["48bc834a572b4caea553ebdf503f2805","47015140fe974c8b93c4892c21f8443a","04e3744ceec747e0991dd1b983e9c182","aec79fd6a7db4eeab3da94a9b9deaee1","e50a9eeaa40f44489e836fecde3f9488","5058110ecc7f4c8aa4503ea66e4b84f3","b0ad911e432742fcb4d1f07f04e164cd","36e5bde718294941a5af2777e868c15d","0e7419e89da14f56aa4b139f0b166aa8","0e2eb3fa60b24fec958cf9a999d7b0de","e508a5b6815e4c24bdcf4159ed0a1cbc","fd0742d6d3654dd2a2c9cc41a0e14c3f","139af509727244b590e605cb5ff3072b","4648acf5c5f446509474d055371d2885","f1b9a86fe0d845a08304399f63d8f2d3","20caba27808345d48e64ffafac8af693","92254b324ed64720aeafc20406227b52","0a53f074e27d4cd685ede72dc713f195","0e936f0da5184904b98cbe53453be9cd","19315383b0a74f7caaf801f12302b1eb","a62c79e1b8114feebc28639702988e5d","e1a628c8050a4eb8b4bda16af0d377d9","5e0391ae83b74f7bb033ea241dce1dcd","20840f2e8dc5442ca3da6ce181648c61","43f11a89945548299b1a1fe70cdd02b8","9a66b09c8fac4847b8fe6dd9264063b3","bcfb4225c10a4584914f48bb464fe673","eb58485470ef458c9467b27feff065f1","fcce2d5ba4464f7e819abaf14983b0c0","e48d9030d6884af78970b1f76b6363d4","3084e1527db24e6a88197360d2c658a1","3f9f2cd1abb54d5cbf72f888b101f395","c53c30ca6eb549cebf68975752ed7df9","2d83a898e1bf4f948bf5129e4ebea6de","dbc603d6b2ae42a5adcece4d6c2ad88a","249c5f6ddc9e41a2972e27b4e67e8aac","dfb189e448634b779d5371273238b634","5f88b0edc3c94a5fbf579d42846963f1","a4036525e2ec405ab2df8086fbe11db0","7fac0e91299d410aaf340e84c8949912"]},"outputId":"03382e2e-8fa3-45c8-8a18-87ad45c17e54"},"source":["train_epochs_roc = 2\n","train(train_epochs=train_epochs_roc, cloze_test=False, batch_size=32, warmup_epochs=0, model_name=ROC_MODEL)\n","test(getModelFileName(ROC_MODEL, train_epochs_roc))"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"48bc834a572b4caea553ebdf503f2805","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0e7419e89da14f56aa4b139f0b166aa8","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=28.0, style=ProgressStyle(description_w…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"92254b324ed64720aeafc20406227b52","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=466062.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"43f11a89945548299b1a1fe70cdd02b8","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=434.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c53c30ca6eb549cebf68975752ed7df9","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1345000548.0, style=ProgressStyle(descr…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForNextSentencePrediction: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n","- This IS expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","  0%|          | 0/2 [00:00<?, ?it/s]"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"2EDcfMlhItBd"},"source":["## ClozeOnly"]},{"cell_type":"code","metadata":{"id":"uWa6H3k_ItMV"},"source":["train_epochs_cloze = 10\n","train(train_epochs = train_epochs_cloze, cloze_test=True, model_name=\"bertfornsp_clozeonly_finetuned\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x1-E16-F984x"},"source":["## Cloze + 5000Roc"]},{"cell_type":"code","metadata":{"id":"V791c8tC-Az6"},"source":["train_epochs_cloze = 5\n","train_mixed(model_name = \"bertfornsp_mixed_more_roc\", train_epochs = train_epochs_cloze)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8CkrBmqpBUQi"},"source":["# Trigger Words"]},{"cell_type":"code","metadata":{"id":"p8SYgheYBWYO"},"source":["def vocab_distribution(dev_only=True, train_only=False, hard = True, token_ids = False):\n","    \"\"\"    \n","    :param token_ids: Return words when False, token_ids when True\n","    :param dev_only: Identify words with high class likelihood in test-set endings\n","    :param train_only: Identify words with high class likelihood in train-set endings (this is where the model gets biased). \n","    :param hard: Use hard test-set (Doesnt exist yet)\n","    \"\"\"\n","    data = []\n","    labels = []\n","    \n","    if dev_only:\n","        cloze = ClozeTest(dev=False, hard = hard)\n","        data.extend(cloze.data)\n","        labels.extend(cloze.labels)\n","\n","    if train_only:\n","        clozedev = ClozeTest(dev=True)\n","        data.extend(clozedev.data)\n","        labels.extend(clozedev.labels)\n","    \n","    tokenizer = BertTokenizer.from_pretrained(BASE_MODEL)\n","\n","\n","    ending_tokens = {}\n","    word_count = 0\n","\n","    for i, story in enumerate(data):\n","        label = labels[i]\n","        end = story[1]    \n","        tokens = tokenizer(end).input_ids\n","        tokens.pop(0)\n","        tokens.pop(-1)\n","        \n","        for token in tokens:\n","            if not token_ids: token = tokenizer.decode(token).replace(\" \", \"\")\n","            word_count += 1\n","            if token not in ending_tokens:\n","                ending_tokens[token] = [0,0]\n","            ending_tokens[token][label] += 1\n","\n","    return ending_tokens, word_count\n","\n","def pmi(class_count, other_class_count, word_count):\n","    \"\"\"\n","    :param class_count: Number of occurences in the class you want to calculate the pmi with\n","    :param other_class_count: Number of occurences in the other class\n","    :param word_count: Total word count\n","    \"\"\"\n","    import math\n","    if class_count < 1:\n","        return 0\n","    return math.log((class_count / word_count) / ((class_count + other_class_count)/(word_count*2)))\n","\n","def class_prob(class_count, other_class_count):\n","    return class_count/(class_count + other_class_count)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YLbJjRF6EgFW"},"source":["def get_trigger_words(hard = False, dev_only = True, train_only = True, min_occurences = 30, token_ids = False):\n","    \"\"\"    \n","    :param token_ids: Return words when False, token_ids when True\n","    :param dev_only: Identify words with high class likelihood in test-set endings\n","    :param train_only: Identify words with high class likelihood in train-set endings (this is where the model gets biased). \n","    :param hard: Use hard test-set (Doesnt exist yet)\n","    :param min_occunrences: Only return trigger words minimally occuring this often\n","    \"\"\"\n","    \n","    vocab_dis, word_count = vocab_distribution(dev_only=dev_only, train_only=train_only, hard=hard, token_ids=token_ids)\n","\n","    pos_triggers = []\n","    neg_triggers = []\n","\n","    for word, dis in vocab_dis.items():\n","        if(dis[0]+dis[1] >= min_occurences):      \n","            pmi_pos = pmi(dis[0], dis[1], word_count)    \n","            pmi_neg = pmi(dis[1], dis[0], word_count)\n","\n","            class_prob_pos = class_prob(dis[0], dis[1])\n","            class_prob_neg = class_prob(dis[1], dis[0])\n","\n","            pos_triggers.append([word, dis[0], pmi_pos, class_prob_pos])\n","            neg_triggers.append([word, dis[1], pmi_neg, class_prob_neg])\n","\n","    pos_triggers.sort(key=lambda x: x[2], reverse = True)\n","    neg_triggers.sort(key=lambda x: x[2], reverse = True)\n","\n","\n","    from tabulate import tabulate\n","    print(tabulate(pos_triggers[:100], headers=['Token', 'n', 'pmi', 'pos_class_likelihood']))\n","    print(\"\\n\")\n","    print(tabulate(neg_triggers[:100], headers=['Token', 'n', 'pmi', 'neg_class_likelihood']))\n","\n","    return pos_triggers, neg_triggers\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZocA9LLRATRs"},"source":["get_trigger_words(hard = False, dev_only = False, train_only = True, min_occurences = 5, token_ids = False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7tMGx20xnPC5"},"source":["# Experiments"]},{"cell_type":"code","metadata":{"id":"9fHALGMn6EkP"},"source":["def test_testset(file, hypothesis_only):\n","    cloze_test = ClozeTest(dev=True, hypothesis_only = hypothesis_only, file=file)\n","    cloze_test_mc = ClozeTest_MC(dev=True, hypothesis_only=hypothesis_only, file=file)\n","    \n","\n","    print(\"\\nBert\\n\")\n","    test(BASE_MODEL, cloze_test = cloze_test)\n","    test_MC(BASE_MODEL, cloze_test = cloze_test_mc)\n","\n","    print(\"\\nRocOnly\\n\")\n","    test(getModelFileName(ROC_MODEL, \"\"), cloze_test = cloze_test)\n","    test_MC(getModelFileName(ROC_MODEL, \"\"), cloze_test = cloze_test_mc)\n","\n","    print(\"\\nClozeOnly\\n\")\n","    test(getModelFileName(\"bertfornsp_clozeonly_finetuned\", \"10\"), cloze_test = cloze_test)\n","    test_MC(getModelFileName(\"bertfornsp_clozeonly_finetuned\", \"10\"), cloze_test = cloze_test_mc)\n","\n","    print(\"\\nRocCloze\\n\")\n","    test(getModelFileName(\"bertfornsp_cloze_finetuned\", \"10\"), cloze_test = cloze_test)\n","    test_MC(getModelFileName(\"bertfornsp_cloze_finetuned\", \"10\"), cloze_test = cloze_test_mc)\n","\n","    print(\"\\nCloze + 5000 Roc\\n\")\n","    test(getModelFileName(\"bertfornsp_mixed\", \"5\"), cloze_test = cloze_test)\n","    test_MC(getModelFileName(\"bertfornsp_mixed\", \"5\"), cloze_test = cloze_test_mc)"],"execution_count":null,"outputs":[]}]}