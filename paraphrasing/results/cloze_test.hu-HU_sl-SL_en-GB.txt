
Bert

Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForNextSentencePrediction: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████| 375/375 [13:57<00:00,  2.23s/it]
              precision    recall  f1-score   support

           0       0.52      0.14      0.23      1871
           1       0.50      0.87      0.64      1871

    accuracy                           0.51      3742
   macro avg       0.51      0.51      0.43      3742
weighted avg       0.51      0.51      0.43      3742

Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForNextSentencePrediction: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
100%|█████████████████████████████████████████████████████████████████████| 188/188 [14:13<00:00,  4.54s/it]
              precision    recall  f1-score   support

           0       0.54      0.54      0.54       960
           1       0.52      0.52      0.52       911

    accuracy                           0.53      1871
   macro avg       0.53      0.53      0.53      1871
weighted avg       0.53      0.53      0.53      1871


RocOnly

100%|█████████████████████████████████████████████████████████████████████| 375/375 [13:34<00:00,  2.17s/it]
              precision    recall  f1-score   support

           0       0.31      0.06      0.10      1871
           1       0.48      0.87      0.62      1871

    accuracy                           0.46      3742
   macro avg       0.40      0.46      0.36      3742
weighted avg       0.40      0.46      0.36      3742

100%|█████████████████████████████████████████████████████████████████████| 188/188 [14:02<00:00,  4.48s/it]
              precision    recall  f1-score   support

           0       0.37      0.36      0.36       960
           1       0.35      0.36      0.35       911

    accuracy                           0.36      1871
   macro avg       0.36      0.36      0.36      1871
weighted avg       0.36      0.36      0.36      1871


ClozeOnly

100%|█████████████████████████████████████████████████████████████████████| 375/375 [13:50<00:00,  2.21s/it]
              precision    recall  f1-score   support

           0       0.81      0.81      0.81      1871
           1       0.81      0.81      0.81      1871

    accuracy                           0.81      3742
   macro avg       0.81      0.81      0.81      3742
weighted avg       0.81      0.81      0.81      3742

100%|█████████████████████████████████████████████████████████████████████| 188/188 [14:15<00:00,  4.55s/it]
              precision    recall  f1-score   support

           0       0.87      0.89      0.88       960
           1       0.88      0.86      0.87       911

    accuracy                           0.87      1871
   macro avg       0.87      0.87      0.87      1871
weighted avg       0.87      0.87      0.87      1871


RocCloze

100%|█████████████████████████████████████████████████████████████████████| 375/375 [13:33<00:00,  2.17s/it]
              precision    recall  f1-score   support

           0       0.82      0.82      0.82      1871
           1       0.82      0.82      0.82      1871

    accuracy                           0.82      3742
   macro avg       0.82      0.82      0.82      3742
weighted avg       0.82      0.82      0.82      3742

100%|█████████████████████████████████████████████████████████████████████| 188/188 [14:07<00:00,  4.51s/it]
              precision    recall  f1-score   support

           0       0.90      0.89      0.90       960
           1       0.89      0.90      0.89       911

    accuracy                           0.89      1871
   macro avg       0.89      0.89      0.89      1871
weighted avg       0.89      0.89      0.89      1871


Cloze + 5000 Roc

100%|█████████████████████████████████████████████████████████████████████| 375/375 [13:38<00:00,  2.18s/it]
              precision    recall  f1-score   support

           0       0.75      0.85      0.80      1871
           1       0.83      0.71      0.77      1871

    accuracy                           0.78      3742
   macro avg       0.79      0.78      0.78      3742
weighted avg       0.79      0.78      0.78      3742

100%|█████████████████████████████████████████████████████████████████████| 188/188 [14:07<00:00,  4.51s/it]
              precision    recall  f1-score   support

           0       0.87      0.88      0.88       960
           1       0.88      0.86      0.87       911

    accuracy                           0.87      1871
   macro avg       0.87      0.87      0.87      1871
weighted avg       0.87      0.87      0.87      1871


