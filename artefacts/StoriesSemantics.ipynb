{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"StoriesSemantics.ipynb","provenance":[{"file_id":"https://github.com/JohannesEschbach/BERT_ROC/blob/johannes/runBertForNextSentencePrediction.ipynb","timestamp":1615753750868}],"collapsed_sections":["VGUHTlObOfPr","EJO2HeysOmYx","l5vXXr3pOpwh","PECOGcFZOx5h","4lr6l5DZIsLN","SXgTW9ZGIshN","2EDcfMlhItBd","YROdTgL0IM3U","8CkrBmqpBUQi","kmPjmLbAgPm7"],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"f256f3892c76443282a7247ffd87b949":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_e761fd5db6ce40b8b6707c3481f48861","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_a214d8347ac743b0b311e971712596bc","IPY_MODEL_0dc40bc7d440471383320dfb1bb6e6b0"]}},"e761fd5db6ce40b8b6707c3481f48861":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a214d8347ac743b0b311e971712596bc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_545647403eb9487b9fdad987b15f8d74","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":434,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":434,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_778b523d731041239283929f5bbfb506"}},"0dc40bc7d440471383320dfb1bb6e6b0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_d515d76f0a0048d69c8126605f4b08d4","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 434/434 [00:01&lt;00:00, 334B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a3bc874c74f346e48d659524995cba7a"}},"545647403eb9487b9fdad987b15f8d74":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"778b523d731041239283929f5bbfb506":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d515d76f0a0048d69c8126605f4b08d4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"a3bc874c74f346e48d659524995cba7a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6138b9e46a56486ab7b72be69dd6f996":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_ac1fef6a8bcb42acb49ace82b0aa750c","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_181b34ce2ae841428632728d573a3632","IPY_MODEL_2f911fdca30f41928bbe1b3a52806b33"]}},"ac1fef6a8bcb42acb49ace82b0aa750c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"181b34ce2ae841428632728d573a3632":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_98067f1e38cd4114b243d8bad901838d","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":1345000548,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1345000548,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b3c27041cfa24e148fe678b9738fb1dd"}},"2f911fdca30f41928bbe1b3a52806b33":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_87f4e7dc8be14e7bb2ba2d48eb79e65e","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1.35G/1.35G [00:39&lt;00:00, 34.1MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0bfd1d712a324c028983df0409f5a710"}},"98067f1e38cd4114b243d8bad901838d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"b3c27041cfa24e148fe678b9738fb1dd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"87f4e7dc8be14e7bb2ba2d48eb79e65e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"0bfd1d712a324c028983df0409f5a710":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"taj1zzZAvY1U","executionInfo":{"status":"ok","timestamp":1616439207891,"user_tz":-60,"elapsed":6835,"user":{"displayName":"Johannes Eschbach","photoUrl":"","userId":"07016017118869064956"}},"outputId":"4420b632-aceb-4c50-d446-4f01bee0a072"},"source":["!pip install transformers"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/d5/f4157a376b8a79489a76ce6cfe147f4f3be1e029b7144fa7b8432e8acb26/transformers-4.4.2-py3-none-any.whl (2.0MB)\n","\r\u001b[K     |▏                               | 10kB 18.1MB/s eta 0:00:01\r\u001b[K     |▎                               | 20kB 20.1MB/s eta 0:00:01\r\u001b[K     |▌                               | 30kB 15.2MB/s eta 0:00:01\r\u001b[K     |▋                               | 40kB 13.6MB/s eta 0:00:01\r\u001b[K     |▉                               | 51kB 9.0MB/s eta 0:00:01\r\u001b[K     |█                               | 61kB 8.5MB/s eta 0:00:01\r\u001b[K     |█▏                              | 71kB 9.6MB/s eta 0:00:01\r\u001b[K     |█▎                              | 81kB 10.6MB/s eta 0:00:01\r\u001b[K     |█▌                              | 92kB 10.1MB/s eta 0:00:01\r\u001b[K     |█▋                              | 102kB 8.7MB/s eta 0:00:01\r\u001b[K     |█▉                              | 112kB 8.7MB/s eta 0:00:01\r\u001b[K     |██                              | 122kB 8.7MB/s eta 0:00:01\r\u001b[K     |██▏                             | 133kB 8.7MB/s eta 0:00:01\r\u001b[K     |██▎                             | 143kB 8.7MB/s eta 0:00:01\r\u001b[K     |██▌                             | 153kB 8.7MB/s eta 0:00:01\r\u001b[K     |██▋                             | 163kB 8.7MB/s eta 0:00:01\r\u001b[K     |██▉                             | 174kB 8.7MB/s eta 0:00:01\r\u001b[K     |███                             | 184kB 8.7MB/s eta 0:00:01\r\u001b[K     |███▏                            | 194kB 8.7MB/s eta 0:00:01\r\u001b[K     |███▎                            | 204kB 8.7MB/s eta 0:00:01\r\u001b[K     |███▌                            | 215kB 8.7MB/s eta 0:00:01\r\u001b[K     |███▋                            | 225kB 8.7MB/s eta 0:00:01\r\u001b[K     |███▉                            | 235kB 8.7MB/s eta 0:00:01\r\u001b[K     |████                            | 245kB 8.7MB/s eta 0:00:01\r\u001b[K     |████▏                           | 256kB 8.7MB/s eta 0:00:01\r\u001b[K     |████▎                           | 266kB 8.7MB/s eta 0:00:01\r\u001b[K     |████▌                           | 276kB 8.7MB/s eta 0:00:01\r\u001b[K     |████▋                           | 286kB 8.7MB/s eta 0:00:01\r\u001b[K     |████▉                           | 296kB 8.7MB/s eta 0:00:01\r\u001b[K     |█████                           | 307kB 8.7MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 317kB 8.7MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 327kB 8.7MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 337kB 8.7MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 348kB 8.7MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 358kB 8.7MB/s eta 0:00:01\r\u001b[K     |██████                          | 368kB 8.7MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 378kB 8.7MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 389kB 8.7MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 399kB 8.7MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 409kB 8.7MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 419kB 8.7MB/s eta 0:00:01\r\u001b[K     |███████                         | 430kB 8.7MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 440kB 8.7MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 450kB 8.7MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 460kB 8.7MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 471kB 8.7MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 481kB 8.7MB/s eta 0:00:01\r\u001b[K     |████████                        | 491kB 8.7MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 501kB 8.7MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 512kB 8.7MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 522kB 8.7MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 532kB 8.7MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 542kB 8.7MB/s eta 0:00:01\r\u001b[K     |█████████                       | 552kB 8.7MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 563kB 8.7MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 573kB 8.7MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 583kB 8.7MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 593kB 8.7MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 604kB 8.7MB/s eta 0:00:01\r\u001b[K     |██████████                      | 614kB 8.7MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 624kB 8.7MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 634kB 8.7MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 645kB 8.7MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 655kB 8.7MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 665kB 8.7MB/s eta 0:00:01\r\u001b[K     |███████████                     | 675kB 8.7MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 686kB 8.7MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 696kB 8.7MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 706kB 8.7MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 716kB 8.7MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 727kB 8.7MB/s eta 0:00:01\r\u001b[K     |████████████                    | 737kB 8.7MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 747kB 8.7MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 757kB 8.7MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 768kB 8.7MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 778kB 8.7MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 788kB 8.7MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 798kB 8.7MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 808kB 8.7MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 819kB 8.7MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 829kB 8.7MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 839kB 8.7MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 849kB 8.7MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 860kB 8.7MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 870kB 8.7MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 880kB 8.7MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 890kB 8.7MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 901kB 8.7MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 911kB 8.7MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 921kB 8.7MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 931kB 8.7MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 942kB 8.7MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 952kB 8.7MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 962kB 8.7MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 972kB 8.7MB/s eta 0:00:01\r\u001b[K     |████████████████                | 983kB 8.7MB/s eta 0:00:01\r\u001b[K     |████████████████                | 993kB 8.7MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 1.0MB 8.7MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 1.0MB 8.7MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 1.0MB 8.7MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 1.0MB 8.7MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 1.0MB 8.7MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 1.1MB 8.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 1.1MB 8.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 1.1MB 8.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 1.1MB 8.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 1.1MB 8.7MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 1.1MB 8.7MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 1.1MB 8.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 1.1MB 8.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 1.1MB 8.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 1.1MB 8.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 1.2MB 8.7MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.2MB 8.7MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.2MB 8.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 1.2MB 8.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 1.2MB 8.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 1.2MB 8.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 1.2MB 8.7MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.2MB 8.7MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.2MB 8.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 1.2MB 8.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 1.3MB 8.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 1.3MB 8.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 1.3MB 8.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.3MB 8.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.3MB 8.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 1.3MB 8.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 1.3MB 8.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 1.3MB 8.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 1.3MB 8.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.4MB 8.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.4MB 8.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 1.4MB 8.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 1.4MB 8.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 1.4MB 8.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 1.4MB 8.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.4MB 8.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.4MB 8.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 1.4MB 8.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 1.4MB 8.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 1.5MB 8.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.5MB 8.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.5MB 8.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.5MB 8.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 1.5MB 8.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 1.5MB 8.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 1.5MB 8.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.5MB 8.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.5MB 8.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.5MB 8.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 1.6MB 8.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 1.6MB 8.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 1.6MB 8.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.6MB 8.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.6MB 8.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.6MB 8.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 1.6MB 8.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.6MB 8.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 1.6MB 8.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.6MB 8.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.7MB 8.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.7MB 8.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.7MB 8.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 1.7MB 8.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 1.7MB 8.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.7MB 8.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.7MB 8.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.7MB 8.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.7MB 8.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.8MB 8.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.8MB 8.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.8MB 8.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.8MB 8.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.8MB 8.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.8MB 8.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.8MB 8.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.8MB 8.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.8MB 8.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.8MB 8.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.9MB 8.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.9MB 8.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 1.9MB 8.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.9MB 8.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.9MB 8.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.9MB 8.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.9MB 8.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.9MB 8.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.9MB 8.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.9MB 8.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 2.0MB 8.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 2.0MB 8.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 2.0MB 8.7MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Collecting tokenizers<0.11,>=0.10.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/23/2ddc317b2121117bf34dd00f5b0de194158f2a44ee2bf5e47c7166878a97/tokenizers-0.10.1-cp37-cp37m-manylinux2010_x86_64.whl (3.2MB)\n","\u001b[K     |████████████████████████████████| 3.2MB 50.6MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.2)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 54.1MB/s \n","\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp37-none-any.whl size=893262 sha256=95c1459ea7c5c681404b2ccbe3de79e8b495f7d9ef88c8f3773e144dd15efe7b\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: tokenizers, sacremoses, transformers\n","Successfully installed sacremoses-0.0.43 tokenizers-0.10.1 transformers-4.4.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H2vOFbYWdhth","executionInfo":{"status":"ok","timestamp":1616439351829,"user_tz":-60,"elapsed":148138,"user":{"displayName":"Johannes Eschbach","photoUrl":"","userId":"07016017118869064956"}},"outputId":"79418bbe-2412-4ddd-8bf7-fec69a274f56"},"source":["from google.colab import drive\n","\n","current_directory = '/content/drive/My Drive/Semantics/BERT_ROC/'\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"VGUHTlObOfPr"},"source":["# Headers and Global Variables"]},{"cell_type":"code","metadata":{"id":"D4MUY1yyup2m"},"source":["import csv\n","import torch\n","from torch.nn.functional import softmax\n","from torch.nn.functional import relu\n","from transformers import BertForNextSentencePrediction, BertTokenizer\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import classification_report\n","from tqdm import tqdm\n","from transformers import AdamW\n","from transformers import get_linear_schedule_with_warmup\n","from matplotlib import pyplot as plt\n","\n","\n","device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n","\n","CLOZE_MODEL = 'bertfornsp_cloze_finetuned'\n","ROC_MODEL = 'bertfornsp_roc_finetuned'\n","# underlying pretrained LM\n","BASE_MODEL = 'bert-large-uncased-whole-word-masking'\n","\n","BATCH_SIZE = 12\n","WARMUP_EPOCHS = 1\n","TRAIN_EPOCHS = 10\n","LAST_EPOCH = -1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EJO2HeysOmYx"},"source":["# Datasets"]},{"cell_type":"code","metadata":{"id":"fdWgHZQtxHi8"},"source":["class RocStories(torch.utils.data.Dataset):\n","    def __init__(self):    \n","        dataset = []       \n","        with open(current_directory + 'roc_stories.csv', \n","                  'r', encoding='utf-8') as d:\n","            \n","            reader = csv.reader(d, quotechar='\"', delimiter=',',\n","                                quoting=csv.QUOTE_ALL, skipinitialspace=True)                \n","            for line in reader:\n","                dataset.append(line)  \n","\n","        self.data = []\n","        self.labels = []\n","\n","        stories = []\n","        endings = []\n","        for sample in dataset:           \n","            start = \" \".join(sample[2:-1])\n","            stories.append(start)            \n","            end = sample[-1]                        \n","            endings.append(end)\n","\n","        from random import shuffle\n","        wrong_endings = endings.copy()\n","        shuffle(wrong_endings)\n","\n","        assert len(stories) == len(endings)\n","        for i, story in enumerate(stories):\n","            \n","            #True Ending\n","            self.data.append([story, endings[i]])\n","            self.labels.append(0)\n","\n","            #Wrong Ending\n","            self.data.append([story, wrong_endings[i]])\n","            self.labels.append(1)\n","\n","    def __getitem__(self, idx):\n","        X = self.data[idx]\n","        y = self.labels[idx]        \n","        return X, y\n","\n","    def __len__(self):\n","        assert len(self.data) == len(self.labels)\n","        return len(self.labels)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N6zAL8cquyF1"},"source":["class ClozeTest(torch.utils.data.Dataset):\n","    def __init__(self, dev=True, hypothesis_only=False, hard = False):\n","        \"\"\"\n","        :param hypothesis_only: Replaces story with empty string. Only Keeps endings as they are.\n","        :param hard: For future hard_test_set.csv\n","        \"\"\"\n","\n","        dataset = []\n","\n","        # if dev=True, we load the dev set for testing\n","        dir = \"\"\n","        if dev:\n","            if hard:\n","                dir = current_directory + 'hard_test_set.csv'\n","            else: \n","                dir = current_directory + 'cloze_test.csv'\n","        else:\n","            dir = current_directory + 'cloze_train.csv'\n","\n","        with open(dir, 'r', encoding='utf-8') as d:\n","            reader = csv.reader(d, quotechar='\"', delimiter=',', \n","                                quoting=csv.QUOTE_ALL, skipinitialspace=True)                \n","            for line in reader:\n","                dataset.append(line) \n","            dataset.pop(0)\n","\n","        self.data = []\n","        self.labels = []\n","\n","        for sample in dataset:\n","            \n","            start = \" \".join(sample[1:-3])\n","            if hypothesis_only: start = \"\"\n","            end1 = sample[-3]\n","            end2 = sample[-2]\n","            right_ending = sample[-1]\n","\n","            self.data.append([start, end1])\n","            self.labels.append(0 if \"1\" == right_ending else 1)\n","\n","            self.data.append([start, end2])\n","            self.labels.append(0 if \"2\" == right_ending else 1)\n","\n","    def __getitem__(self, idx):\n","        X = self.data[idx]\n","        y = self.labels[idx]        \n","        return X, y\n","\n","    def __len__(self):\n","        assert len(self.data) == len(self.labels)\n","        return len(self.labels)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l5vXXr3pOpwh"},"source":["# Auxiliary Functions"]},{"cell_type":"code","metadata":{"id":"dw0X81uWJVwd"},"source":["def getModelFileName(model_name, last_epoch):\n","    return current_directory + model_name + str(last_epoch)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BgItyJtueXHR"},"source":["def weight_diff(model1, model2):\n","    diff = torch.nn.MSELoss() # diff(a, b) = ((a - b) ** 2).mean()\n","\n","    xweights, yweights, xbiases, ybiases = dict(), dict(), dict(), dict()\n","    layer_names = set()\n","\n","    for (name, parameter1), parameter2 in zip(\n","        model1.bert.encoder.layer.named_parameters(),\n","        model2.bert.encoder.layer.parameters()\n","    ):\n","\n","        difference = diff(parameter1, parameter2).item()\n","\n","        name = name.split(\".\")\n","        xtick = float(name[0])\n","        layer_name = \".\".join(name[1:-1])\n","        parameter_type = name[-1]\n","\n","        if layer_name not in layer_names:\n","            layer_names.add(layer_name)\n","            xweights[layer_name], xbiases[layer_name] = list(), list()\n","            yweights[layer_name], ybiases[layer_name] = list(), list()\n","\n","        if parameter_type == \"weight\":\n","            yweights[layer_name].append(difference)\n","            xweights[layer_name].append(xtick + 0.0)\n","        else: # if parameter_type == \"bias\"\n","            ybiases[layer_name].append(difference)\n","            xbiases[layer_name].append(xtick + 0.5)\n","\n","    for name in layer_names:\n","        plt.bar(xweights[name], yweights[name], width=0.4, label=\"weight\")\n","        plt.bar(xbiases[name], ybiases[name], width=0.4, label=\"bias\")\n","        plt.xticks(xweights[name])\n","        plt.legend()\n","        plt.title(name)\n","        plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PECOGcFZOx5h"},"source":["# Functions for Training and Testing"]},{"cell_type":"code","metadata":{"id":"_yCLlZ7UPpfa"},"source":["def train(cloze_test, model_file=BASE_MODEL, batch_size=BATCH_SIZE,\n","          warmup_epochs=WARMUP_EPOCHS, train_epochs=TRAIN_EPOCHS,\n","          last_epoch=LAST_EPOCH, verbose=False):\n","    \n","    tokenizer = BertTokenizer.from_pretrained(BASE_MODEL)\n","    model = BertForNextSentencePrediction.from_pretrained(model_file)\n","    # The old weights are saved in model_old to be used to compare to model\n","    model_old = BertForNextSentencePrediction.from_pretrained(model_file)\n","\n","    #Send to GPU and allow Training\n","    model = model.to(device)\n","    model.train()\n","\n","    trainloader = torch.utils.data.DataLoader(\n","        ClozeTest(dev=False) if cloze_test else RocStories(),\n","        batch_size=batch_size, shuffle=True\n","    )\n","\n","    #LR maybe needs to be optimized\n","    optimizer = AdamW(model.parameters(), lr=1e-5)\n","    n_batches =  len(trainloader)\n","    scheduler = get_linear_schedule_with_warmup(\n","        optimizer,\n","        num_warmup_steps=(warmup_epochs * n_batches),\n","        num_training_steps=(train_epochs * n_batches),\n","        last_epoch=max(-1, last_epoch * n_batches) # actually, last_step\n","    )\n","    losses = []\n","\n","    epochs_range = range(last_epoch + 1, train_epochs)\n","    for epoch in tqdm(epochs_range):\n","        \n","        for batchId, (stories, labels) in zip(range(n_batches), trainloader):\n","            # this is PyTorch-specific as gradients get accumulated        \n","            optimizer.zero_grad()\n","\n","            start = stories[0]\n","            end = stories[1]\n","\n","            labels = labels.to(device)\n","           \n","            # Tokenize sentence pairs.\n","            # All sequences in batch processing must be same length.\n","            # Therefore we use padding to fill shorter sequences\n","            # with uninterpreted [PAD] tokens)\n","            tokenized_batch = tokenizer(start, padding = True, text_pair = end,\n","                                        return_tensors='pt').to(device)\n","            \n","            loss = model(**tokenized_batch, labels = labels).loss\n","            if verbose:\n","                print(\"Epoch \" + str(epoch + 1) + \n","                      \" Batch \" + batchId + \" of \" + n_batches + \n","                      \" Loss: \" + loss.item())\n","            losses.append(loss.item())\n","\n","            loss.backward()\n","            optimizer.step()\n","            scheduler.step() # Huggingface specific: step = epoch\n","\n","        model.save_pretrained(\n","            getModelFileName((CLOZE_MODEL if cloze_test \n","                              else ROC_MODEL), epoch + 1)\n","        )\n","    \n","    # Loss function change over steps is plotted below.\n","    plt.plot(losses)\n","    plt.xticks(\n","        ticks=[(i - last_epoch - 1) * n_batches for i in epochs_range],\n","        labels=epochs_range\n","    )\n","    plt.title((\"Story Cloze\" if cloze_test else \"ROCStories\") + \" Training\")\n","    plt.xlabel(\"Epoch\")\n","    plt.ylabel(\"Loss\")\n","    plt.show()\n","\n","    # Models are compared\n","    weight_diff(model, model_old)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ypcXSNFzu0Ds"},"source":["def test(model_file=BASE_MODEL, verbose = False, cloze_test = ClozeTest()):\n","    softmax = torch.nn.Softmax(dim=1)\n","    tokenizer = BertTokenizer.from_pretrained(BASE_MODEL)\n","    model = BertForNextSentencePrediction.from_pretrained(model_file)\n","\n","    #Send to GPU and allow Evaluation\n","    model = model.to(device)\n","    model.eval()\n","\n","    #Dataloader\n","    devloader = torch.utils.data.DataLoader(cloze_test, batch_size=10)\n","\n","    pred_list, label_list = list(), list()\n","\n","    for stories, labels in tqdm(devloader, disable=verbose):\n","        \n","        start = stories[0]\n","        end = stories[1]\n","        \n","        # Tokenize sentence pairs.\n","        # All sequences in batch processing must be same length.\n","        # Therefore we use padding to fill shorter sequences\n","        # with uninterpreted [PAD] tokens)\n","        tokenized_batch = tokenizer(start, padding = True, text_pair = end,\n","                                    return_tensors='pt').to(device)\n","\n","        #Send to GPU\n","        labels = labels.to(device)\n","\n","        outputs = model(**tokenized_batch, labels = labels)\n","        logits = outputs.logits\n","\n","        # Model predicts sentence-pair as correct if True-logit > False-logit\n","        predictions = logits.argmax(dim=1).int()\n","        probs = softmax(logits).cpu().detach()\n","\n","        # Extra info print() if verbose\n","        if verbose:\n","            # iterate over elements in batch\n","            for i, element_input_ids in enumerate(tokenized_batch.input_ids):\n","                print(tokenizer.decode(element_input_ids))\n","                print(\"Probability:\", probs[i][0].item() * 100)\n","                print(\"Predicted: \", bool(predictions[i]))\n","                print(\"True label: \", bool(labels[i]))\n","\n","        pred_list.extend(predictions.tolist())\n","        label_list.extend(labels.tolist())\n","\n","    print(confusion_matrix(label_list, pred_list))\n","    print(classification_report(label_list, pred_list))\n","\n","    return confusion_matrix(label_list, pred_list).ravel()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4lr6l5DZIsLN"},"source":["# Testing Pretrained BertForNextSentencePrediction"]},{"cell_type":"code","metadata":{"id":"pEFH-RCZIsWV","colab":{"base_uri":"https://localhost:8080/","height":390,"referenced_widgets":["f256f3892c76443282a7247ffd87b949","e761fd5db6ce40b8b6707c3481f48861","a214d8347ac743b0b311e971712596bc","0dc40bc7d440471383320dfb1bb6e6b0","545647403eb9487b9fdad987b15f8d74","778b523d731041239283929f5bbfb506","d515d76f0a0048d69c8126605f4b08d4","a3bc874c74f346e48d659524995cba7a","6138b9e46a56486ab7b72be69dd6f996","ac1fef6a8bcb42acb49ace82b0aa750c","181b34ce2ae841428632728d573a3632","2f911fdca30f41928bbe1b3a52806b33","98067f1e38cd4114b243d8bad901838d","b3c27041cfa24e148fe678b9738fb1dd","87f4e7dc8be14e7bb2ba2d48eb79e65e","0bfd1d712a324c028983df0409f5a710"]},"executionInfo":{"status":"ok","timestamp":1615830694420,"user_tz":-60,"elapsed":98958,"user":{"displayName":"Johannes Eschbach","photoUrl":"","userId":"07016017118869064956"}},"outputId":"d36185ce-2302-41a9-a474-cbc64f24a5d4"},"source":["test()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f256f3892c76443282a7247ffd87b949","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=434.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6138b9e46a56486ab7b72be69dd6f996","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1345000548.0, style=ProgressStyle(descr…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForNextSentencePrediction: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n","- This IS expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","100%|██████████| 375/375 [00:55<00:00,  6.71it/s]"],"name":"stderr"},{"output_type":"stream","text":["[[ 348 1523]\n"," [ 298 1575]]\n","              precision    recall  f1-score   support\n","\n","           0       0.54      0.19      0.28      1871\n","           1       0.51      0.84      0.63      1873\n","\n","    accuracy                           0.51      3744\n","   macro avg       0.52      0.51      0.46      3744\n","weighted avg       0.52      0.51      0.46      3744\n","\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"SXgTW9ZGIshN"},"source":["# Training and Testing the Model on ROCStories Dataset"]},{"cell_type":"code","metadata":{"id":"60O5LnU2IsyG","colab":{"base_uri":"https://localhost:8080/","height":357},"executionInfo":{"status":"error","timestamp":1616252029214,"user_tz":-60,"elapsed":67904,"user":{"displayName":"Johannes Eschbach","photoUrl":"","userId":"07016017118869064956"}},"outputId":"17694e75-978c-4bce-deed-b2d9e6db65c3"},"source":["train_epochs_roc = 1\n","#train(train_epochs=train_epochs_roc, cloze_test=False)\n","test(getModelFileName(ROC_MODEL, train_epochs))"],"execution_count":null,"outputs":[{"output_type":"stream","text":[" 59%|█████▉    | 221/375 [00:33<00:22,  6.83it/s]"],"name":"stderr"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-52-bf5bf007879f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_epochs_roc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#train(train_epochs=train_epochs_roc, cloze_test=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetModelFileName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bertfornsp_finetuned\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-51-b523c20f8059>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(model_file, verbose)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# with uninterpreted [PAD] tokens)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         tokenized_batch = tokenizer(start, padding = True, text_pair = end,\n\u001b[0;32m---> 25\u001b[0;31m                                     return_tensors='pt').to(device)\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m#Send to GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2264\u001b[0m                 \u001b[0mreturn_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2265\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2266\u001b[0;31m                 \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2267\u001b[0m             )\n\u001b[1;32m   2268\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mbatch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2449\u001b[0m             \u001b[0mreturn_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2450\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2451\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2452\u001b[0m         )\n\u001b[1;32m   2453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    523\u001b[0m                 \u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpair_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mids_or_pair_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m             \u001b[0mfirst_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m             \u001b[0msecond_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpair_ids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecond_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mget_input_ids\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m                 \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_tokens_to_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0mno_split_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique_no_split_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m         \u001b[0mtokenized_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_on_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno_split_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtokenized_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36msplit_on_tokens\u001b[0;34m(tok_list, text)\u001b[0m\n\u001b[1;32m    334\u001b[0m                     (\n\u001b[1;32m    335\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique_no_split_tokens\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m                         \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenized_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m                     )\n\u001b[1;32m    338\u001b[0m                 )\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    334\u001b[0m                     (\n\u001b[1;32m    335\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique_no_split_tokens\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m                         \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenized_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m                     )\n\u001b[1;32m    338\u001b[0m                 )\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/tokenization_bert.py\u001b[0m in \u001b[0;36m_tokenize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0msplit_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_basic_tokenize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasic_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnever_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_special_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                 \u001b[0;31m# If the token is part of the never_split set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/tokenization_bert.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, never_split)\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;31m# union() returns a new set by concatenating the two sets.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0mnever_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnever_split\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnever_split\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnever_split\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnever_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clean_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0;31m# This was added on November 1st, 2018 for the multilingual and Chinese\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/tokenization_bert.py\u001b[0m in \u001b[0;36m_clean_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    498\u001b[0m                 \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m                 \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    501\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"2EDcfMlhItBd"},"source":["# Training and Testing the Model Further on StoryCloze Dataset"]},{"cell_type":"code","metadata":{"id":"uWa6H3k_ItMV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616365216949,"user_tz":-60,"elapsed":61765,"user":{"displayName":"Johannes Eschbach","photoUrl":"","userId":"07016017118869064956"}},"outputId":"f6ce29fd-a9d0-43bc-dfda-0fc75817e57a"},"source":["train_epochs_cloze = 10\n","#train(model_file=getModelFileName(ROC_MODEL, train_epochs_roc), cloze_test=True)\n","test(getModelFileName(CLOZE_MODEL, train_epochs_cloze))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|██████████| 375/375 [00:50<00:00,  7.43it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["[[1671  200]\n"," [ 346 1525]]\n","              precision    recall  f1-score   support\n","\n","           0       0.83      0.89      0.86      1871\n","           1       0.88      0.82      0.85      1871\n","\n","    accuracy                           0.85      3742\n","   macro avg       0.86      0.85      0.85      3742\n","weighted avg       0.86      0.85      0.85      3742\n","\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["array([1671,  200,  346, 1525])"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"markdown","metadata":{"id":"YROdTgL0IM3U"},"source":["#Saliency Maps"]},{"cell_type":"code","metadata":{"id":"ysbV0jP3IO4R"},"source":["#Old Version. Check Kamal's branch for up to date version\n","\n","def saliency_map(model, tokenizer, input, ending, label):\n","\n","    \n","    # Activations are saved.\n","    acts = dict() # one-key dictionary. Doesn't work otherwise.\n","    def get_acts(name):\n","        def hook(module, input, output):\n","\t\t\t      acts[name] = output.detach()\n","        return hook\n","\n","\t  # Gradients are saved.\n","    grads = dict() # same as for activations\n","    def get_grads(name):\n","\t\t    def hook(module, input, output):\n","\t\t\t      grads[name] = output[0].detach() # 'output' is a tuple\n","\t\t    return hook\n","\n","    frw_handle = model.bert.embeddings.register_forward_hook(get_acts(\"emb\"))\n","    bck_handle = model.bert.embeddings.register_backward_hook(get_grads(\"emb\"))\n","    \n","    tokens = tokenizer(input, text_pair=ending, return_tensors='pt').to(device)\n","\n","    token_names = tokenizer.decode(tokens.input_ids[0])\n","\n","\n","    model.eval()\n","    model.zero_grad()\n","    model = model.to(device)\n","\n","    model(**tokens, labels = torch.tensor(label).to(device)).loss.backward()\n","\n","    frw_handle.remove()\n","    bck_handle.remove()\n","\n","    saliencies = (-grads[\"emb\"] * acts[\"emb\"]).sum(dim=-1)\n","    saliencies = relu(saliencies) # relu'd saliencies\n","    saliencies = saliencies / saliencies.max() # normalization\n","    saliencies = saliencies[0] # squeezing the batch of one\n","    return list(zip(token_names.split(), saliencies.tolist())) # token-saliency pairs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZzoNcQZ8Lq90"},"source":["model = BertForNextSentencePrediction.from_pretrained(getModelFileName(CLOZE_MODEL, \"10\"))\n","tokenizer = BertTokenizer.from_pretrained(BASE_MODEL)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QMCa0UhuIUAE"},"source":["cloze_test = ClozeTest(dev=True)\n","story, label = cloze_test[43] #Example data point\n","input, ending = story\n","\n","saliency_map(model = model, tokenizer=tokenizer, input = input, ending = ending, label = label)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8CkrBmqpBUQi"},"source":["#Trigger Words"]},{"cell_type":"code","metadata":{"id":"p8SYgheYBWYO","colab":{"base_uri":"https://localhost:8080/","height":146},"executionInfo":{"status":"error","timestamp":1616444071594,"user_tz":-60,"elapsed":464,"user":{"displayName":"Johannes Eschbach","photoUrl":"","userId":"07016017118869064956"}},"outputId":"aa834564-9ff6-455b-a77b-4bc4c871a172"},"source":["def vocab_distribution(dev_only=True, train_only=False, hard = True, token_ids = False):\n","    \"\"\"    \n","    :param token_ids: Return words when False, token_ids when True\n","    :param dev_only: Identify words with high class likelihood in test-set endings\n","    :param train_only: Identify words with high class likelihood in train-set endings (this is where the model gets biased). \n","    :param hard: Use hard test-set (Doesnt exist yet)\n","    \"\"\"\n","    data = []\n","    labels = []\n","    \n","    if dev_only:\n","        cloze = ClozeTest(dev=False, hard = hard)\n","        data.extend(cloze.data)\n","        labels.extend(cloze.labels)\n","\n","    if train_only:\n","        clozedev = ClozeTest(dev=True)\n","        data.extend(clozedev.data)\n","        labels.extend(clozedev.labels)\n","    \n","    tokenizer = BertTokenizer.from_pretrained(BASE_MODEL)\n","\n","\n","    ending_tokens = {}\n","    word_count = 0\n","\n","    for i, story in enumerate(data):\n","        label = labels[i]\n","        end = story[1]    \n","        tokens = tokenizer(end).input_ids\n","        tokens.pop(0)\n","        tokens.pop(-1)\n","        \n","        for token in tokens:\n","            if not token_ids: token = tokenizer.decode(token).replace(\" \", \"\")\n","            word_count += 1\n","            if token not in ending_tokens:\n","                ending_tokens[token] = [0,0]\n","            ending_tokens[token][label] += 1\n","\n","    return ending_tokens, word_count\n","\n","def pmi(class_count, other_class_count, word_count):\n","    \"\"\"\n","    :param class_count: Number of occurences in the class you want to calculate the pmi with\n","    :param other_class_count: Number of occurences in the other class\n","    :param word_count: Total word count\n","    \"\"\"\n","    import math\n","    if class_count < 1:\n","        return 0\n","    return math.log((class_count / word_count) / ((class_count + other_class_count)/(word_count*2)))\n","\n","def class_prob(class_count, other_class_count):\n","    return class_count/(class_count + other_class_count)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-15-df15aa4b6274>\"\u001b[0;36m, line \u001b[0;32m51\u001b[0m\n\u001b[0;31m    return class_count/(class_count + other_class_count)\u001b[0m\n\u001b[0m                                                        \n^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOF while scanning triple-quoted string literal\n"]}]},{"cell_type":"code","metadata":{"id":"YLbJjRF6EgFW"},"source":["def get_trigger_words(hard = False, dev_only = True, train_only = True, min_occurences = 30, token_ids = False):\n","    \"\"\"    \n","    :param token_ids: Return words when False, token_ids when True\n","    :param dev_only: Identify words with high class likelihood in test-set endings\n","    :param train_only: Identify words with high class likelihood in train-set endings (this is where the model gets biased). \n","    :param hard: Use hard test-set (Doesnt exist yet)\n","    :param min_occunrences: Only return trigger words minimally occuring this often\n","    \"\"\"\n","    \n","    vocab_dis, word_count = vocab_distribution(dev_only=dev_only, train_only=train_only, hard=hard, token_ids=token_ids)\n","\n","    pos_triggers = []\n","    neg_triggers = []\n","\n","    for word, dis in vocab_dis.items():\n","        if(dis[0]+dis[1] >= min_occurences):      \n","            pmi_pos = pmi(dis[0], dis[1], word_count)    \n","            pmi_neg = pmi(dis[1], dis[0], word_count)\n","\n","            class_prob_pos = class_prob(dis[0], dis[1])\n","            class_prob_neg = class_prob(dis[1], dis[0])\n","\n","            pos_triggers.append([word, dis[0], pmi_pos, class_prob_pos])\n","            neg_triggers.append([word, dis[1], pmi_neg, class_prob_neg])\n","\n","    pos_triggers.sort(key=lambda x: x[2], reverse = True)\n","    neg_triggers.sort(key=lambda x: x[2], reverse = True)\n","\n","\n","    from tabulate import tabulate\n","    print(tabulate(pos_triggers[:100], headers=['Token', 'n', 'pmi', 'pos_class_likelihood']))\n","    print(\"\\n\")\n","    print(tabulate(neg_triggers[:100], headers=['Token', 'n', 'pmi', 'neg_class_likelihood']))\n","\n","    return pos_triggers, neg_triggers\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZocA9LLRATRs"},"source":["get_trigger_words(hard = False, dev_only = False, train_only = True, min_occurances = 10, token_ids = False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kmPjmLbAgPm7"},"source":["#Hard Testset (OUTDATED, SOME TRIES, NOTHING RELEVANT, MAYBE GOOD FOR COPYING SOME CODE FOR FUTURE PURPOSES)"]},{"cell_type":"code","metadata":{"id":"TfYNbG5SgaKS"},"source":["def eliminate_hyp_only_stories(cloze_test = ClozeTest(dev=True, hypothesis_only=True)):\n","    tokenizer = BertTokenizer.from_pretrained(BASE_MODEL)\n","    model = BertForNextSentencePrediction.from_pretrained(getModelFileName(CLOZE_MODEL, 10))\n","\n","    #Send to GPU and allow Evaluation\n","    model = model.to(device)\n","    model.eval()\n","   \n","    labels = cloze_test.labels\n","\n","    biased = []\n","\n","    for i, sample in enumerate(cloze_test.data):\n","        label = torch.LongTensor([labels[i]])\n","        label = label.to(device)\n","        certainty = predict(\"\", sample[1], label, tokenizer, model)\n","        biased.append([i, certainty]) #append index\n","\n","    biased.sort(key=lambda x: x[1], reverse = True)\n","\n","    from tabulate import tabulate\n","    print(tabulate(biased, headers=['Story', 'certainty']))\n","\n","\n","    biased_endings = [elem[0] for elem in biased]\n","\n","    biased_stories = []\n","    for i in biased_endings:\n","        biased_stories.append(i)\n","        if i % 2 == 0: biased_stories.append(i+1)\n","        else: biased_stories.append(i-1)\n","\n","    return biased_stories\n","\n","def remove_words(bias_tokens, file):\n","    tokenizer = BertTokenizer.from_pretrained(BASE_MODEL)\n","    with open(current_directory + 'hard_test_set.csv', 'a') as f:\n","        with open(current_directory + file, 'r', encoding='utf-8') as d:\n","            reader = csv.reader(d, quotechar='\"', delimiter=',', quoting=csv.QUOTE_ALL, skipinitialspace=True)                \n","            for sample in reader:\n","                ends = [sample[-3],sample[-2]]\n","                new_ends = []\n","                for end in ends:\n","                    tokens = tokenizer(end).input_ids\n","\n","                    reduced_end = []\n","                    for token in tokens:\n","                        if token not in bias_tokens:\n","                            word = tokenizer.decode(token).replace(\" \", \"\")   \n","                            reduced_end.append(word)\n","                    new_ends.append(\" \".join(reduced_end))\n","                #...\n","\n","\n","                    \n","                    \n","            \n","                \n","\n","\n","\n","    return\n","\n","def create_reduced_testset(remove_ind):\n","    with open(current_directory + 'hard_test_set.csv', 'a') as f:\n","    with open(current_directory + 'cloze_test.csv', 'r', encoding='utf-8') as d:\n","        lines = d.readlines()\n","        lines.pop(0)\n","        f.write(\"header line \\n\")\n","        for i, line in enumerate(lines):\n","            if i * 2 not in remove_ind:\n","                f.write(line)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GM9GqYrrEqsA"},"source":["triggers = get_trigger_words(hard = False, dev_only = False, train_only = True, min_occurances = 3, token_ids = True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JRYBZVjODMfL"},"source":["story_indices = eliminate_hyp_only_stories()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LDEnKsB0uujy"},"source":["cloze_test = ClozeTest(dev=True,hypothesis_only=True)\n","remove_ind = story_indices[:2000]\n","cloze_test.data = [x for i, x in enumerate(cloze_test.data) if i not in remove_ind]\n","cloze_test.labels = [x for i, x in enumerate(cloze_test.labels) if i not in remove_ind]\n","\n","#tn, fp, fn, tp = test(model_file = getModelFileName(CLOZE_MODEL, \"10\"), cloze_test=cloze_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6av6k1J_voAq"},"source":["create_reduced_testset(remove_ind)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wtJggI0o2qBH"},"source":["cloze_test_hard = ClozeTest(dev=True, hard=True, hypothesis_only=True)\n","test(model_file = getModelFileName(CLOZE_MODEL, \"10\"), cloze_test=cloze_test_hard)\n","cloze_test_hard = ClozeTest(dev=True, hard=True)\n","test(model_file = getModelFileName(CLOZE_MODEL, \"10\"), cloze_test=cloze_test_hard)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cOFjqSFQE2kG"},"source":["def train_bias_reduced(cloze_test, model_file=BASE_MODEL, batch_size=BATCH_SIZE,\n","          warmup_epochs=WARMUP_EPOCHS, train_epochs=TRAIN_EPOCHS,\n","          last_epoch=LAST_EPOCH, verbose=False):\n","    \n","    tokenizer = BertTokenizer.from_pretrained(BASE_MODEL)\n","    model = BertForNextSentencePrediction.from_pretrained(model_file)\n","    # The old weights are saved in model_old to be used to compare to model\n","    model_old = BertForNextSentencePrediction.from_pretrained(model_file)\n","\n","    #Send to GPU and allow Training\n","    model = model.to(device)\n","    model.train()\n","\n","    trainloader = torch.utils.data.DataLoader(\n","        ClozeTest(dev=False) if cloze_test else RocStories(),\n","        batch_size=batch_size, shuffle=True\n","    )\n","\n","    #LR maybe needs to be optimized\n","    optimizer = AdamW(model.parameters(), lr=1e-5)\n","    n_batches =  len(trainloader)\n","    scheduler = get_linear_schedule_with_warmup(\n","        optimizer,\n","        num_warmup_steps=(warmup_epochs * n_batches),\n","        num_training_steps=(train_epochs * n_batches),\n","        last_epoch=max(-1, last_epoch * n_batches) # actually, last_step\n","    )\n","    losses = []\n","\n","    epochs_range = range(last_epoch + 1, train_epochs)\n","    for epoch in tqdm(epochs_range):\n","        \n","        for batchId, (stories, labels) in zip(range(n_batches), trainloader):\n","            # this is PyTorch-specific as gradients get accumulated        \n","            optimizer.zero_grad()\n","\n","            start = stories[0]\n","            end = stories[1]\n","\n","            labels = labels.to(device)\n","           \n","            # Tokenize sentence pairs.\n","            # All sequences in batch processing must be same length.\n","            # Therefore we use padding to fill shorter sequences\n","            # with uninterpreted [PAD] tokens)\n","            tokenized_batch = tokenizer(start, padding = True, text_pair = end,\n","                                        return_tensors='pt').to(device)\n","            \n","            loss = model(**tokenized_batch, labels = labels).loss\n","            if verbose:\n","                print(\"Epoch \" + str(epoch + 1) + \n","                      \" Batch \" + batchId + \" of \" + n_batches + \n","                      \" Loss: \" + loss.item())\n","            losses.append(loss.item())\n","\n","            loss.backward()\n","            optimizer.step()\n","            scheduler.step() # Huggingface specific: step = epoch\n","\n","        model.save_pretrained(\n","            getModelFileName((CLOZE_MODEL if cloze_test \n","                              else ROC_MODEL), epoch + 1)\n","        )\n","    \n","    # Loss function change over steps is plotted below.\n","    plt.plot(losses)\n","    plt.xticks(\n","        ticks=[(i - last_epoch - 1) * n_batches for i in epochs_range],\n","        labels=epochs_range\n","    )\n","    plt.title((\"Story Cloze\" if cloze_test else \"ROCStories\") + \" Training\")\n","    plt.xlabel(\"Epoch\")\n","    plt.ylabel(\"Loss\")\n","    plt.show()\n","\n","    # Models are compared\n","    weight_diff(model, model_old)"],"execution_count":null,"outputs":[]}]}